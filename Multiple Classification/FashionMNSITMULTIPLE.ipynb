{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLwJOOcphdJ9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform= transforms.Compose([transforms.ToTensor()])\n",
        "train_data= datasets.FashionMNIST(root='data',train=True,download=True,transform=transform)\n",
        "test_data= datasets.FashionMNIST(root='data',train=False,download=True,transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2za_956hzCt",
        "outputId": "5b86a690-10fd-4c81-bc01-72975dc9f6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 201kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.74MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 19.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size =32\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=batch_size)"
      ],
      "metadata": {
        "id": "vLAm9s0EjfxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "print(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK6IDHBukPDe",
        "outputId": "a4b379f9-a3e4-4eec-9403-ff9437f03fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images,labels = next(iter(train_loader))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(9):\n",
        "  plt.subplot(3,3,i+1)\n",
        "  plt.imshow(images[i].squeeze(),cmap='gray')\n",
        "  plt.title(class_names[labels[i]])\n",
        "  plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "id": "RYd392ZgkhHG",
        "outputId": "9919e6eb-3bb5-446f-cf92-a940b43ec461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28])\n",
            "torch.Size([32])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxkAAAMsCAYAAAA4VG/hAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAbtdJREFUeJzt/Xl4VfXV//+vkORkHiBkYBACAURARRHBW2SQISpqUVTAiUGBKth6ffXu3dbbim3VOhbFgvBpxam9HcGRQVSsE7QKBUULMgsCIUBC5nn//vDHKSG41gn7nYTA83FdXpec1z77vM+w39kr++S9wjzP8wQAAAAAHGnR1AMAAAAAcGKhyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAIAT3LZt2yQsLEweeeQRc9sZM2ZIWFhYI4wKAHAio8hoAmFhYSH99+GHHzb1UAE0guY6J5SUlMiMGTPUceXl5UlERIS8/PLLIiJy//33y+uvv944AwRwzJ555pla8090dLS0bdtWsrOz5YknnpDCwsKmHiKOcxFNPYCT0fPPP1/r388995wsW7aszu2nnXZaYw4LQBM5nuaE//3f/5Vf/vKXIW1bUlIi9957r4iIDB48+KjbLF26VMLCwmTEiBEi8kORcdVVV8moUaNcDBdAA/vtb38rnTp1ksrKStmzZ498+OGHcvvtt8tjjz0mb775ppxxxhlNPUQcpygymsD1119f698rV66UZcuW1bn9SCUlJRIbG9uQQ2sQxcXFEhcX19TDAI5bxzonNISIiAiJiNB/NNTU1EhFRUVI+1u0aJGcf/75kpyc7GB0ABrbxRdfLOecc07w37/61a/kgw8+kEsvvVQuv/xy+fe//y0xMTFHvS8//09ufF3qODV48GDp1auXrFq1SgYOHCixsbHy61//WkRE9u7dKzfddJOkp6dLdHS0nHnmmfLss8/Wuv+HH3541K9XHPpu9jPPPBO8bc+ePTJx4kRp3769REVFSZs2beQnP/mJbNu2rdZ9Fy9eLBdccIHExcVJQkKCjBw5Ur7++uta20yYMEHi4+Nl8+bNcskll0hCQoJcd911zl4XAHV98cUXkp2dLa1bt5aYmBjp1KmTTJo06ajbzps3T7KysiQqKkr69u0rn3/+ea38aH+TERYWJtOnT5e//vWv0rNnT4mKipKnnnpKUlNTRUTk3nvvDX6lYsaMGcH71dTUyJIlS2TkyJHB/RQXF8uzzz4b3H7ChAnB7f/1r3/JxRdfLImJiRIfHy9Dhw6VlStX1hrLoa9wfPTRRzJ16lRJSUmRxMREufHGGyUvL+9YX0IA9XDhhRfK3XffLdu3b5cXXnhBRPSf/zU1NTJz5kzp2bOnREdHS3p6ukydOrXOMRvKXPbiiy9Knz59JCEhQRITE+X000+Xxx9/vHGeOOqFKxnHsf3798vFF18sY8eOleuvv17S09OltLRUBg8eLJs2bZLp06dLp06d5JVXXpEJEyZIfn6+/PznP6/344wePVq+/vprue222yQzM1P27t0ry5Ytk++++04yMzNF5Ievc4wfP16ys7PlwQcflJKSEpkzZ44MGDBA/vWvfwW3ExGpqqqS7OxsGTBggDzyyCPN8uoL0Fzs3btXRowYIampqfLLX/5SkpOTZdu2bbJgwYI62/7tb3+TwsJCmTp1qoSFhclDDz0kV155pWzZskUiIyPVx/nggw/k5ZdflunTp0vr1q3lzDPPlDlz5sgtt9wiV1xxhVx55ZUiIrW+OvH5559Lbm6uXHLJJSLywzxy8803y7nnnitTpkwREZGsrCwREfn666/lggsukMTERPnFL34hkZGRMnfuXBk8eLD8/e9/l379+tUaz/Tp0yU5OVlmzJghGzZskDlz5sj27duDv2AB0LBuuOEG+fWvfy3vvvuuTJ48WUR+/Of/1KlT5ZlnnpGJEyfKz372M9m6das8+eST8q9//Us+/fRTiYyMDGkuW7ZsmYwbN06GDh0qDz74oIiI/Pvf/5ZPP/30mM5/0MA8NLlp06Z5R74VgwYN8kTEe+qpp2rdPnPmTE9EvBdeeCF4W0VFhXfeeed58fHxXkFBged5nrd8+XJPRLzly5fXuv/WrVs9EfHmz5/veZ7n5eXleSLiPfzwwz86vsLCQi85OdmbPHlyrdv37NnjJSUl1bp9/Pjxnoh4v/zlL0N+/gBqO9qc8GMWLlzoiYj3+eef/+g2h477lJQU78CBA8Hb33jjDU9EvLfeeit42z333FPnsUXEa9Gihff111/Xuj03N9cTEe+ee+456uPefffdXseOHWvdFhcX540fP77OtqNGjfICgYC3efPm4G27du3yEhISvIEDBwZvmz9/viciXp8+fbyKiorg7Q899JAnIt4bb7zxo68DgNAdOta0uSUpKck766yzPM/78Z//H3/8sSci3l//+tdaty9ZsqTW7aHMZT//+c+9xMREr6qq6lifFhoRX5c6jkVFRcnEiRNr3bZo0SLJyMiQcePGBW+LjIyUn/3sZ1JUVCR///vf6/UYMTExEggE5MMPP/zRrxosW7ZM8vPzZdy4cbJv377gf+Hh4dKvXz9Zvnx5nfvccsst9RoHgGNz6G8d3n77bamsrFS3HTNmjLRs2TL47wsuuEBERLZs2WI+zqBBg6RHjx71GtuiRYuCX5XSVFdXy7vvviujRo2Szp07B29v06aNXHvttfLJJ59IQUFBrftMmTKl1tWXW265RSIiImTRokX1GiOAYxcfH19nlakjf/6/8sorkpSUJMOHD691DtGnTx+Jj48PnkOEMpclJydLcXGxLFu2zP2TgXMUGcexdu3aSSAQqHXb9u3bpWvXrtKiRe237tCqM9u3b6/XY0RFRcmDDz4oixcvlvT0dBk4cKA89NBDsmfPnuA2GzduFJEfvoOZmppa6793331X9u7dW2ufERER0r59+3qNA4CuqKhI9uzZE/wvNzdXRH44+R89erTce++90rp1a/nJT34i8+fPl/Ly8jr76NChQ61/Hyo4Qvlbhk6dOtVrvHv27JHVq1eHVGTk5uZKSUmJnHrqqXWy0047TWpqamTHjh21bu/atWutf8fHx0ubNm3q/C0ZgIZTVFQkCQkJwX8f7ef/xo0b5eDBg5KWllbnHKKoqCh4DhHKXHbrrbdKt27d5OKLL5b27dvLpEmTZMmSJY3zZFFv/E3GcezHVmsIxY99J7m6urrObbfffrtcdtll8vrrr8vSpUvl7rvvlgceeEA++OADOeuss6SmpkZEfvg+dUZGRp37H7kSTVRUVJ0iCIA/jzzySHC5WBGRjh07BhdyePXVV2XlypXy1ltvydKlS2XSpEny6KOPysqVKyU+Pj54n/Dw8KPu2/M88/HrOx8tXrxYoqOjZciQIfW6H4DmYefOnXLw4EHp0qVL8Laj/fyvqamRtLQ0+etf/3rU/RxaQCKUuSwtLU3WrFkjS5culcWLF8vixYtl/vz5cuONN9ZZAAdNjyKjmenYsaN8+eWXUlNTU+tAXr9+fTAX+c9vKPPz82vd/8eudGRlZckdd9whd9xxh2zcuFF69+4tjz76qLzwwgvBP8xMS0uTYcOGuX5KAEJw4403yoABA4L/PvKkv3///tK/f3+577775G9/+5tcd9118uKLL8rNN9/cYGPS/sD6nXfekSFDhtQZ59Huk5qaKrGxsbJhw4Y62fr166VFixZyyimn1Lp948aNtQqYoqIi2b17d/CPzAE0rEN9fLKzs9XtsrKy5L333pPzzz8/pF9WWHNZIBCQyy67TC677DKpqamRW2+9VebOnSt33313rYIHTY9fNzczl1xyiezZs0deeuml4G1VVVUya9YsiY+Pl0GDBonID8VGeHi4fPTRR7XuP3v27Fr/LikpkbKyslq3ZWVlSUJCQvASZXZ2tiQmJsr9999/1O9JHvraBoCG07lzZxk2bFjwv/PPP19Efviq05FXInr37i0ictSvTLl0aOWYI3+ZUVlZKcuWLTvqV6Xi4uLqbB8eHi4jRoyQN954o9bXnXJycuRvf/ubDBgwQBITE2vdZ968ebXmozlz5khVVZVcfPHF/p4UANMHH3wgv/vd76RTp07mMvXXXHONVFdXy+9+97s6WVVVVXA+CGUu279/f628RYsWwRXtGnq+Q/1xJaOZmTJlisydO1cmTJggq1atkszMTHn11Vfl008/lZkzZwa/G5mUlCRXX321zJo1S8LCwiQrK0vefvvtOn8/8e2338rQoUPlmmuukR49ekhERIQsXLhQcnJyZOzYsSIikpiYKHPmzJEbbrhBzj77bBk7dqykpqbKd999J++8846cf/758uSTTzb6awFA5Nlnn5XZs2fLFVdcIVlZWVJYWCj/7//9P0lMTGzw3+rHxMRIjx495KWXXpJu3bpJq1atpFevXpKbmysFBQVHLTL69Okj7733njz22GPStm1b6dSpk/Tr109+//vfy7Jly2TAgAFy6623SkREhMydO1fKy8vloYceqrOfioqK4Ny1YcMGmT17tgwYMEAuv/zyBn3OwMlm8eLFsn79eqmqqpKcnBz54IMPZNmyZdKxY0d58803JTo6Wr3/oEGDZOrUqfLAAw/ImjVrZMSIERIZGSkbN26UV155RR5//HG56qqrQprLbr75Zjlw4IBceOGF0r59e9m+fbvMmjVLevfuHfzbVBxHmnh1K3g/voRtz549j7p9Tk6ON3HiRK9169ZeIBDwTj/99OCStIfLzc31Ro8e7cXGxnotW7b0pk6d6q1bt67WErb79u3zpk2b5nXv3t2Li4vzkpKSvH79+nkvv/xynf0tX77cy87O9pKSkrzo6GgvKyvLmzBhgvfFF18Etxk/frwXFxd37C8GgHotYbt69Wpv3LhxXocOHbyoqCgvLS3Nu/TSS2sdl4eWsD3aUtVyxBK0P7aE7bRp0476+J999pnXp08fLxAIBPd15513ej169Djq9uvXr/cGDhzoxcTEeCJSaznb1atXe9nZ2V58fLwXGxvrDRkyxPvss89q3f/Qspp///vfvSlTpngtW7b04uPjveuuu87bv3+/9XIBCNGhY+3Qf4FAwMvIyPCGDx/uPf7448El8w+xfv7PmzfP69OnjxcTE+MlJCR4p59+uveLX/zC27Vrl+d5oc1lr776qjdixAgvLS3NCwQCXocOHbypU6d6u3fvbpgXAb6EeV4If/EHAECIevToIZdeeulRr0D4daih1+effy7nnHOO8/0DANzg61IAAGcqKipkzJgxcs011zT1UAAATYgiAwDgTCAQkHvuuaephwEAaGKsLgUAAADAKf4mAwAAAIBTXMkAAAAA4BRFBgAAAACnKDIAAAAAOBXy6lJhYWENOY7jwqFu2T/m1VdfVfMtW7aoeWRkpJqnpKSo+cqVK9W8a9euar5jxw41FxGzc2e7du3UvKysTM2Li4vVPD8/X83vvfdeNT8ZNOc/o2roeaRFC/33JjU1NQ36+C5YzyExMVHN09LS1Hzs2LFqvnbtWjX/xz/+oeYi9nM4Wifww3Xu3FnNn3jiCTW35pHS0lI1Pxk013nkZDgXsfid55KTk9XcOg/Ys2ePmruYh/3uIykpSc2t8y3rfA6hzSFcyQAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcCrkJWxPBqeccoqaZ2RkqPnq1avV3FoybdSoUWretm1bNbeWqLX2LyLy8MMPq7m1BG1mZqaaV1ZWqnlcXJyaW8sMFxYWqjlObMfDErXWPHLaaaepeWxsrJrv27dPzTt27Kjmt956q5pv3LjR1+OLiOTl5an5ZZddpuYLFy5Uc2ue6N+/v5pbS20XFBSoubXMb1FRkZoDfvhd3tVaIvqKK65Q87vvvlvNj4flka+++mo1t+YxlrB1gysZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcCrMC3FB47CwsIYeS5MbPny4mt94441q/vbbb6v5gAED1HzDhg1q3qFDBzW3lJeXm9vcfvvtav7MM8+oudUrJDk52RyDZunSpWr+zTff+Np/c3A8rEF+rJrDPGL1Yhk0aJCaV1RUqPnBgwfV3DpOrTXyt2/frubvvfeemlt9PEIRFRWl5tZrcOaZZ6p5dHS0msfExKh5RITeIsr6DLRq1UrN16xZo+YiIps3bza3aUjNdR5pDnNIQ7NeA7/v7Z133qnmjzzyiK/9h4eHm9tYvT6s5zhu3Dg1t87X6LllC+VzxpUMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOKUvFn6Sad++vZrn5+ereWRkpJpv2rRJzcvKytR8xYoVan7eeeep+QcffKDmIvb69ZdddpmaL1iwQM0zMzPVvLKyUs2BhmYdR3l5eb7yQCDgK7f6cMTHx6v5wIED1dzqRdOrVy81FxEpLS1Vc2sese7fpk0bcwwaa57Zu3evmlvv8RlnnGGOYffu3WpeUlJi7gMnp4bucXLttdequXUu8umnn6p5dXV1vcd0pLS0NDUfNWqUmv/f//2f7zHAxpUMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOBXmhbjgclhYWEOPpck98cQTam6ta75lyxY1j46OVnOrz4bf+4ey7npBQYGab9y4Uc3Hjh2r5nFxcb7y4uJiNb/lllvU/ETQ0GukN6SmnkeSkpLMbfr27avmBw4cUPOICL39kNXnwmLt3+q3Y/WAKCoqUvNLLrlEzUVEPvvsMzW35pkuXbqoeVVVlZpbvUZqamrUPDw8XM2tz7E1V4vYx7HVa8Cv5jqPNPUccjywXgO/763VUys1NVXNP/nkEzUP5bNt9eMZMmSImlvH+A033KDmVl8zhPY540oGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOCU3tXpJGM1ybKaXFm51cDGagBlKSwsVPOEhARzH9Y21nO0GuBkZmaqufUc9uzZo+aApmXLlr73YTW9rKysVHOrgZHVbM9qRGft3zoGq6ur1Xz58uVqLiKSkZGh5mlpaWpuPceoqCg1t56DpbS0VM1btNB/P2eNT0QkMTGxXmMCDmnoZnyxsbFqbh0fw4YNU/OLLrqo3mM6Uk5Ojppbx6g1R9GMzw2uZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMAp+mQcxlrb3OoRYa3L7LfPhtXDwlq/Pzc3V81DER0dreZffvmlmr///vtqbvUSCaXXB/Bj2rVrZ25j9Wiwejxs3bq1XmM6knWMWWvkW6xeNlaPia5du5qPYc1lVi8Ray6z+gBY6/jHxMSoeatWrdTceo9C6VNg9WWy+mgUFBSYjwEcjdUHwzoXsubI/fv3q7n12Rfx/7PemkNc9EyCjSsZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIo+GfUQHh6u5hER+stZXFys5tba636Fsn9rjXxr7em4uDhf+x86dKiaX3DBBWo+Z84cNcfJLT4+3tymvLxcza014s8//3w1//jjj9XcmicCgYCaW6weFtY8Zj3/UPZh9RSyenlY81BKSoqal5SUqLnVJ8Oa56z9i9i9AqzXECcvv71y2rZtq+Z++2RYx2coc5h1jFnzmPUaWcc43OBKBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJxiIe7DFBUVqbm19vr+/fvVPJT15TWVlZVqbq1NnZyc7OvxRey1qa01/q3X8MUXX1TzBQsWqDlOblYvm1DWZ7c+w4WFhWr+k5/8RM0LCgrUfNWqVWpurR9vzTPWGviWUPo3+B2D9T5Zc6HVgyItLU3N09PT1dzqpZKXl6fmInbfotTUVDU/cOCA+Rg4MXme5+v+p5xyippbx7jVg8I6PkOZh61eOX57hSQmJvq6P0LDlQwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZ+Mw1jr41dXV6u5te55SUmJrzwhIcHX41u5iN3rI5R9aPLz89XcWr9748aNvh4fJzarF0xpaam5D6vHg7W++sqVK9X81FNPVfPPPvtMzePj49Xc6uFg9aiw+gVZa+CL2OvsW+v8W8/B2r+1Dr/VY+Kiiy5S8y+//FLNrV4oInYfAGu+B45Vhw4d1LxFC/33z9bxa+XW/kPZh9Unwzq+rD40cIMrGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKPhmH+f7779W8W7duar5+/Xo1t9ZuLysrU3PLoEGD1Nxa/z6UMezZs6deYzrSli1b1NzqIRAbG+vr8XFiS09PV/NQ+rxYvTaSkpLUvFOnTmo+Y8YMNbeeg7U+fEVFhZpbfUCs/Vs9KkTsXhpWzyG/fTCsNfJ3796t5v/4xz/UfNSoUWq+c+dONRfx3+8EOFZ++2Q0dB+NULbxOwZrnocbXMkAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAU/TJOMzGjRvV/IwzzlBza+12a11n6/7W+vefffaZmo8dO1bNRUTCw8PV3OolYj2H1NRUcwyagwcP+ro/TmzffPONmm/atMnch/UZHTlypK/HOHDggJp37dpVza1+N1aPCKsPhpVbfTZCERkZqebWXGk9R2uNfOs9tubSPn36qLn1HouIrFu3Ts1LS0vNfeDkZH3+LZmZmb7277ePhtUnR8R/nwxL27Ztfd0foeFKBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiT8ZhSkpK1Dw5OVnNrbXfKysr1dzqg2E9/pw5c9TcWttdROS0005T808//dTchx/R0dFqXlBQ0KCPjxObdYyJ2L1g5s2b52sMWVlZam6tD2/lFr/3d8Eag7VOv9XPx3qf4+Pj1XzNmjVq/otf/ELNgeNZz5491dzqhRMRoZ86lpWV+dq/iD0HWGOwzres8ym4wZUMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWfjMPk5ub6ur/VJ8Pv2u7V1dX1HtPhQnl+p556qpqH0mdAY/XBCAQCvvYP+NWihf67F2v9dovVj8da/92aZ6x5wnp+Vh4WFqbmInYfDGsfVl5aWurr8ZsD6zU4EZ4jmkZGRoaaFxYWqrk1B1lzpDXHhMLahzUPtm7d2vcYYONKBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiT8ZhiouL1dzq8eB3/XxrffyUlBRf+//+++993V/EXl87Pz/f1/6tPhxWjwHAL+s49tu/wFq/3cqtY8SaRyzW87f6/YSyj8rKSjWPiopSc+s1ttbxt/LjAX0w0FAOHDig5tYcFxMTo+bHQ58Mi9/zKYSGKxkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFM04zuM1UjOauCUnJys5nv37lVzq4lWQkKCmls2b97s6/6hsJrwFBQUqHnPnj3V3GqYCDS0hm6SZs0zgUBAza15pLS0VM2t51dVVaXmoWwTHx+v5lajLes1aGpWMzMRmu2h4aSlpal5YmKimufl5am534afjfHZt85FYmNj1dxqOmo1TcUPuJIBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAAp+iTcRhrXWUrt9Z2LysrU/O4uDg192vHjh3mNjExMWreqlUrNS8sLFRza31ta/17a//A8c7vGvOVlZVqbs1TFqtPRyhr3CclJam51UfCWqO+qKhIza3XyG/PIeB41r59ezX322fGOj6tOS6UXjt++1BYj2Gdr2VlZan5t99+W+8xnYy4kgEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACn6JNRD3v27FHzli1bqrnftamttd8tofSYKC0tVfP4+Hg1T05OVnPrOVhrY1u9RoDjnbXGvHUMWn0qrB4UVm71yQhljXu/c5UlOjpaza15wuolYr1GofQKAZqK1afG6mPht9eOi+PH2ofFegz6ZDQOrmQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKfpk1ENFRUWT7t9vj4jc3FxzG2sNfGt9+v3796u51QejodfvBpqa1SfD6jFhHSPW+vLWMWTNM9b4Q2GtUV9UVKTm1mvg9zW2ehqVl5erOdCUMjIy1Nzvz1nr+Doe+sxYY7DOt9q1a+dyOCctrmQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKfpkOGT1gLDExcWpeUlJia/9h3J/a/36lJQUNU9OTlZza/1ta318oLmLiopSc2t9d789HqzcmgNCYfXbsfgdg/UcrX4/kZGRak6fDBzPrJ/D1hzjN7eOX6tPh4jdS8Pq1eG3p5Y1TyM0XMkAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAU/TJqIf09HQ1z83NVfP8/Hw1j42N9ZW7YK3Bb60Pv2vXLjW3noO1fj3Q3Fmfcb89Iqz15a1j3FoD39q/iH2cW2vcW/1yrDGUlZWpudUHw+qzARzPGrpPhsVFD4uG7tVh5YmJiWqO0HAlAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE7RJ6MerLXZKyoq1Ly4uFjNrfXzrf27UF1drebW+vXW+vPW+t3WawQ0d9YxVlJSouZVVVW+Ht9aXz4iwv+PBasXh5Vbr5G1Dn9UVJSaW2vkh7KOP3C8sno8hNLrxg/r+HTx+H77CVnHOH0y3OBKBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiT0Y9WGu3W30uEhISXA6nQVhrR1vrX1t9MqzXyOoRAJzsrGPIWj/e6pNhseZBEf+9NgKBgK/9Wz2FSktL1dyaxywN3YcA0KSmpqq5NQc09Oc3lDmooecxS0Pv/2TBlQwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZ+MerB6RFhru1us9e/Lysp87T8UVVVVam6N8cCBA2puvUbW2thAc2cdA1FRUb7273eNe2t9eL89MEJh9evxOxdaz8Ga54DjWXJysq/7W3OAlbv4Od7Q5wLWHON3HsYPOKMDAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiGV89VFRUqHl1dbWaW838rPs3hqKiIjXPz89X84yMDDVv1aqVmlsNcoDmLjIyUs2Li4vVPDY21tfj+23WZ41fxG7qabHmAb/7t5rx0RQUzVlSUlKTPr51rhPK8dXQTT+tMSQmJjbo458smEkBAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFH0y6iE3N1fNs7Ky1HzHjh1qnpCQoOZr1qxRcxes9efPPPNMNbfW+E9NTVXz999/X82B5i4sLEzNrWPQyq315a3Ht/poVFZWqrmISHl5uZpbfTCsMVq51XPIGp+1f+B4ZvWpsI5xv/2qrMcvKCgw92H1qbD6XFhzgNUTLDo6Ws0RGq5kAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCn6ZNRDZGSkmicnJ6u51WfDWt++pKREzV1o06aNmlvrZ8fHx/u6/759+9QcaO6s49w6hqw17q314a017K0eETExMWouIhIIBNTceg5RUVFqfvDgQTW3nmNsbKyal5aWqjlwPOvUqZOaW3NMUlKSmrdq1UrNU1JSfN1fRKR169Zqbh2j1v2t3HqNEBquZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMAp+mTUQ35+vpqXlZWpudVno6qqSs13796t5i588803at6jRw81t9a3t9bYb4znCDQlv/OI1QPC6kFhsfp4WD0qQhmD1cvDGoOVW69hcXGxmufl5ak5cDx79tln1bxnz55qHh0drebnnnuump9zzjlqftFFF6m5iMiiRYvUfPjw4Wpu9RVLT09X8xUrVqg5QsOVDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADgV5vldVB0AAAAADsOVDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZGBoG3btklYWJg88sgjTT0UAI0kLCxMZsyYEfz3M888I2FhYbJt27YmGxOAkxfnIicOioxG9tVXX8lVV10lHTt2lOjoaGnXrp0MHz5cZs2a1dRDA9AMHCoCDv0XHR0t3bp1k+nTp0tOTk5TDw9AM8C5CBpDRFMP4GTy2WefyZAhQ6RDhw4yefJkycjIkB07dsjKlSvl8ccfl9tuu62phwigmfjtb38rnTp1krKyMvnkk09kzpw5smjRIlm3bp3ExsY29fAAHKc4F0FjochoRPfdd58kJSXJ559/LsnJybWyvXv3Ns2gGllJSQknQIADF198sZxzzjkiInLzzTdLSkqKPPbYY/LGG2/IuHHjmnh0Dae4uFji4uKaehhAs8W5COcijYWvSzWizZs3S8+ePesc1CIiaWlpwf8PCwuT6dOny+uvvy69evWSqKgo6dmzpyxZsqTO/b7//nuZNGmSpKenB7d7+umna21TUVEhv/nNb6RPnz6SlJQkcXFxcsEFF8jy5cvNMXueJ1OmTJFAICALFiwI3v7CCy9Inz59JCYmRlq1aiVjx46VHTt21Lrv4MGDpVevXrJq1SoZOHCgxMbGyq9//WvzMQHU34UXXigiIlu3bpXBgwfL4MGD62wzYcIEyczMPKb9z549W3r27ClRUVHStm1bmTZtmuTn5wfz6dOnS3x8vJSUlNS577hx4yQjI0Oqq6uDty1evFguuOACiYuLk4SEBBk5cqR8/fXXdcYbHx8vmzdvlksuuUQSEhLkuuuuO6bxA/gB5yKcizQWioxG1LFjR1m1apWsW7fO3PaTTz6RW2+9VcaOHSsPPfSQlJWVyejRo2X//v3BbXJycqR///7y3nvvyfTp0+Xxxx+XLl26yE033SQzZ84MbldQUCB//vOfZfDgwfLggw/KjBkzJDc3V7Kzs2XNmjU/Oobq6mqZMGGCPPfcc7Jw4UK58sorReSH34LceOON0rVrV3nsscfk9ttvl/fff18GDhxY66RDRGT//v1y8cUXS+/evWXmzJkyZMiQer1mAEKzefNmERFJSUlxvu8ZM2bItGnTpG3btvLoo4/K6NGjZe7cuTJixAiprKwUEZExY8ZIcXGxvPPOO7XuW1JSIm+99ZZcddVVEh4eLiIizz//vIwcOVLi4+PlwQcflLvvvlu++eYbGTBgQJ0/OK+qqpLs7GxJS0uTRx55REaPHu38+QEnE85FOBdpNB4azbvvvuuFh4d74eHh3nnnnef94he/8JYuXepVVFTU2k5EvEAg4G3atCl429q1az0R8WbNmhW87aabbvLatGnj7du3r9b9x44d6yUlJXklJSWe53leVVWVV15eXmubvLw8Lz093Zs0aVLwtq1bt3oi4j388MNeZWWlN2bMGC8mJsZbunRpcJtt27Z54eHh3n333Vdrf1999ZUXERFR6/ZBgwZ5IuI99dRT9X2pAPyI+fPneyLivffee15ubq63Y8cO78UXX/RSUlK8mJgYb+fOnd6gQYO8QYMG1bnv+PHjvY4dO9a6TUS8e+65p87+t27d6nme5+3du9cLBALeiBEjvOrq6uB2Tz75pCci3tNPP+15nufV1NR47dq180aPHl1r/y+//LInIt5HH33keZ7nFRYWesnJyd7kyZNrbbdnzx4vKSmp1u3jx4/3RMT75S9/Wd+XCcCP4FwEjYUrGY1o+PDhsmLFCrn88stl7dq18tBDD0l2dra0a9dO3nzzzVrbDhs2TLKysoL/PuOMMyQxMVG2bNkiIj9cOnzttdfksssuE8/zZN++fcH/srOz5eDBg7J69WoREQkPD5dAICAiIjU1NXLgwAGpqqqSc845J7jN4SoqKuTqq6+Wt99+WxYtWiQjRowIZgsWLJCamhq55ppraj1mRkaGdO3atc5lz6ioKJk4caKbFxBA0LBhwyQ1NVVOOeUUGTt2rMTHx8vChQulXbt2Th/nvffek4qKCrn99tulRYv//MiYPHmyJCYmBq9chIWFydVXXy2LFi2SoqKi4HYvvfSStGvXTgYMGCAiIsuWLZP8/HwZN25crTkkPDxc+vXrd9SvTtxyyy1OnxNwMuNcBI2FP/xuZH379pUFCxZIRUWFrF27VhYuXCh//OMf5aqrrpI1a9ZIjx49RESkQ4cOde7bsmVLycvLExGR3Nxcyc/Pl3nz5sm8efOO+liH/wHXs88+K48++qisX78++PUGEZFOnTrVud8DDzwgRUVFsnjx4jrf6964caN4niddu3Y96mNGRkbW+ne7du2CkwoAd/70pz9Jt27dJCIiQtLT0+XUU0+tVQS4sn37dhEROfXUU2vdHggEpHPnzsFc5IevTM2cOVPefPNNufbaa6WoqEgWLVokU6dOlbCwMBH5YQ4R+c/fkBwpMTGx1r8jIiKkffv2zp4PAM5F0DgoMppIIBCQvn37St++faVbt24yceJEeeWVV+See+4REQl+d/lInueJyA+/BRARuf7662X8+PFH3faMM84QkR/+MGrChAkyatQo+e///m9JS0uT8PBweeCBB4Lf4z5cdna2LFmyRB566CEZPHiwREdHB7OamhoJCwuTxYsXH3WM8fHxtf4dExNjvRQAjsG5554bXF3qSGFhYcG54nCH/+F1Q+jfv79kZmbKyy+/LNdee6289dZbUlpaKmPGjAluc2juev755yUjI6POPiIiav9YioqKapDiCQDnImhYFBnHgUMnCrt37w75PqmpqZKQkCDV1dUybNgwddtXX31VOnfuLAsWLAj+NlFEgpPIkfr37y8//elP5dJLL5Wrr75aFi5cGPzBn5WVJZ7nSadOnaRbt24hjxdA42nZsmXw6wyHO/yqQ6g6duwoIiIbNmyQzp07B2+vqKiQrVu31pl/rrnmGnn88celoKBAXnrpJcnMzJT+/fsH80NfvUhLSzPnLgCNh3MRuMavhxrR8uXLj/rbxUWLFolI3a8jaMLDw2X06NHy2muvHXWFiNzc3Frbikitx/7HP/4hK1as+NH9Dxs2TF588UVZsmSJ3HDDDcHfVlx55ZUSHh4u9957b53n4nlerRUnADSNrKwsWb9+fa15YO3atfLpp5/We1/Dhg2TQCAgTzzxRK1j/i9/+YscPHhQRo4cWWv7MWPGSHl5uTz77LOyZMkSueaaa2rl2dnZkpiYKPfff3+tr0sccviYAbjHuQgaC1cyGtFtt90mJSUlcsUVV0j37t2loqJCPvvss+Bv++r7R0l/+MMfZPny5dKvXz+ZPHmy9OjRQw4cOCCrV6+W9957Tw4cOCAiIpdeeqksWLBArrjiChk5cqRs3bpVnnrqKenRo0etP9A80qhRo2T+/Ply4403SmJiosydO1eysrLk97//vfzqV7+Sbdu2yahRoyQhIUG2bt0qCxculClTpsidd97p63UC4M+kSZPksccek+zsbLnppptk79698tRTT0nPnj2loKCgXvtKTU2VX/3qV3LvvffKRRddJJdffrls2LBBZs+eLX379pXrr7++1vZnn322dOnSRe666y4pLy+v9VUpkR/+5mLOnDlyww03yNlnny1jx46V1NRU+e677+Sdd96R888/X5588knfrwGAo+NcBI2mcRezOrktXrzYmzRpkte9e3cvPj7eCwQCXpcuXbzbbrvNy8nJCW4nIt60adPq3L9jx47e+PHja92Wk5PjTZs2zTvllFO8yMhILyMjwxs6dKg3b9684DY1NTXe/fff73Xs2NGLioryzjrrLO/tt9+us5zl4cvGHW727NmeiHh33nln8LbXXnvNGzBggBcXF+fFxcV53bt396ZNm+Zt2LAhuM2gQYO8nj17HuvLBeAoDi0x+/nnn6vbvfDCC17nzp29QCDg9e7d21u6dOkxLWF7yJNPPul1797di4yM9NLT071bbrnFy8vLO+pj33XXXZ6IeF26dPnR8S1fvtzLzs72kpKSvOjoaC8rK8ubMGGC98UXXwS3GT9+vBcXF6c+TwD1w7kIGkuY5x3lmhkAAAAAHCP+JgMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOhdzxOywsrCHHASBEzbm1DfOIf71791bz0aNHq3lWVpaaV1RUqHlOTo6ai4h069ZNzcvLy9V87Nix5mNorM9Zcz6GXGmurwFzCHB8CGUO4UoGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTIS9hCwDHu8ZYurRFC/13MzU1Nb4fQzNmzBg179ixo5pv3rxZzfv166fm5557rpqLiISHh6v5unXr1NxapnfNmjVqbr3P1ufE7+eouS4PCwAucSUDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAAToV5IS7oba0bDqBxNOc1+JvDPNIYvTY0p5xyipq/9NJLav7uu++qeZs2bdT84MGDan7gwAE1FxGJjY1V86qqKjUvLi5W80cffdQcQ0Nq6s+IC81hjEfTHOYQ4GQQyhzClQwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZ8MoJlpruvbi5wY80hcXJyaDx06VM1PP/10NU9KSlJzq89Ft27d1Nzqg7F9+3Y1b9HC/t1USUmJmldUVKh5ZWWlmhcWFqr5rl271Hzbtm1q/sUXX/h6/Oaguc4jJ8IcApwI6JMBAAAAoNFRZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWfDKCZaa7r24s0/Dxi7T+U1y4zM1PN77//fjXPzc1V83379vm6v8V6Dfbv36/mXbt2VfPo6GhzDGVlZWpu9dGwxpienq7miYmJal5TU6PmaWlpan7XXXep+YEDB9RcxO43Yo3Rr+Y6j3AuAhwf6JMBAAAAoNFRZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMApmvEBzUxzbaIl0jzmkVmzZqm51Whu06ZNah4bG1vvMR0uKSlJzUtLS33lxcXFal5dXa3mIvb7fMopp6h5Tk6OmluvYShj1ERFRal5UVGRms+ZM8fX4zeG5jqPNIc5BDgZ0IwPAAAAQKOjyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcCqiqQdwIrHW727odcnDw8PV3O/a8ccDv6+x1WPgN7/5jTmGN998U83//ve/m/tA04iLizO3iYmJUfOSkhI1T01NVXOrz0ZFRYWaW8dAVVWVr/snJCSoeU1NjZqLiLRoof/+Kj4+Xs137Nih5tHR0WpuPUfrOVhzaffu3dXcGp+I/TkAgOaOKxkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwij4Z9WCt/W5p6D4ZjdEHY+jQoWp+1113qfmFF16o5tZrbK1fX1lZqeZ33HGHmt9www1qLiJywQUXqPnw4cPV/ODBg2purfGPY3fmmWea20RFRal5ly5d1HzTpk31GtORysvL1dzqwZCfn6/mVo8I6/MXyjxjzXXWY/ida4uKitS8TZs2at6yZUs1t3qZZGRkqLmIyLZt28xtAKA540oGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKJPRj1Y68tbIiMj1dzq8WCZNWuWmmdmZpr7iIuLU/P09HQ1j4mJMR9DY73Gft+DSy+9VM137dpl7sN6juPHj1fzJ554Qs0bup/KyaxHjx7mNlafidatW6v5wIED1fz1119X89jYWDW3ejjk5uaqudXnIhAIqHkoSktLfd3f6kNh5VYfDutzsH37djW3+niEMtfSJwMNxfr8N4efMWlpaWr+l7/8Rc1HjRql5o3RVwxcyQAAAADgGEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBT9MloRH77YNx0001qPnXqVDX/7rvvzMew1rePiNA/MmVlZWo+Z84cNd+5c6eaV1VVqbnVB8Pq81FUVKTmIvb62pMmTVJzq08GGk6bNm3MbaxeLNZnpHPnzmpu9Vmx1oe37t+2bVs1t/qAWD0orDlARCQ6OlrNreMwJSVFza1jsFu3bmqekJCg5nl5eWpujS8qKkrNgYbU0H0wrD4xfvtZidjnCr1791bzDz74QM0HDRpU3yHhGHAlAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE7RJ+M48uqrr6q51QNi69atal5cXGyOITY2Vs3DwsLU3Fp//qc//ak5Bj+sPh8HDx5U80Ag4PsxevTooebJyclqbvUxwLHr2LGjuY3VI8HqBWMdI+Hh4Wpu9YAoKCjwdX+rz4Xf+4vY6+Tv2bNHzRMTE9U8JydHza11/C1WnwHrPY6Pj/f1+EBTsj7ffvtgnHvuueY2DzzwgJpffPHFan7llVeq+a5du9TcmkOs84Dy8nJfuTUPi9i9z6x+RR06dFBz61wmFFzJAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFPNpk+Gi7XZrXWPq6qq6jWmI/Xu3VvNP/74YzXft2+fmm/YsEHNo6Ki1DyUteNjYmLU3Fo/vrCwUM03b96s5pWVlWpuPcfIyEg1t56fdX8Rew3xLVu2qLnV7+SFF14wx4BjExcXZ25jHYdWnwurR4K1drl1DFjzlLW+ut95MpT12635uqSkRM2tNeStY9Dqh9O2bVs1t/rlWONr2bKlmgNNyTo+/Z4LWf0VioqKzH08/fTTaj5p0iQ1t3pxWOcyVj+kUM5JNdY8G8r5Wvv27dXcmie3b9/uewwWrmQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAqUbrk2Gt1+tibXZr3WNrfXjLvHnz1Hz06NFqvn//fjXfu3evmltruxcXF6u5tT6/iP0+WetbJyYmqrn1Pu/atUvNTzvtNF/7t9b/DmVdaGsNfetz9l//9V9qTp+MhhMbG2tuY/WpsOYiv71YrM+P9fmzxmfNk377bISyD6tfiTVXWv1yrNfImqdSUlLUvKKiQs0TEhLUHNBYP4esY9jK/fbB6N69u5r/f//f/6fmVn8HEZGtW7eq+VdffaXm69atU3NrHrb6HfmdRy3W/kXs99Hax1lnnaXmHTp0MMdg4UoGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnHLWJ8Pqr9DQawqL2Ose33rrrWp+8803q3mvXr3U/JtvvlFza335tLQ0Nbd6VFhrJltrw4uIlJeXq3lEhP6RSU1NVfPc3Fw1T09PV3PrOe7YsUPNrbWvQ1nf3vosW++T1UcBDSeUPijh4eFqXlBQoObWcW59xvLz89Xcmmut3GI9/1D4nc+t47xVq1ZqvnbtWjW/6KKL1Nx6j/Ly8tQ8lJ5EOD6FMkdY/J4P+e3pZfnd736n5nPnzlXzpUuXqvlzzz2n5ueff76ai9jnQ59++qmaWz2RrF43Vq8di/U5ss6lQvkcWs/R+llina8lJyebY7BwJQMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOOeuTYa37bOndu7ea//znPzf3MWzYMDVv3769mm/cuFHNv/jiCzVv27atmpeVlam5tba2tTa8df/CwkI1F7HXjrbWv7fWZY6Li1PzpKQkNbee42mnnabmVp8Mq8eFiP0aWe/zhg0bzMdAwwilR0lpaamad+3aVc3/9a9/qbm19rj1GbTWT7fW2LeOIWv/ofThsOYJq6eRtYa8df8DBw6ouTU+q8+F39cQDcf67FjHR0P3qHBhzJgxan7VVVf5yq05avv27Wo+YcIENd+8ebOai9jnClYfDYvfOcTq5RMVFaXm1nlEKJ9D65zOmof27t2r5oMGDTLHYGEmBAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcctaMr3Xr1mo+f/58Nbea8YXS7K+kpETNv/rqKzW3GixZz9FqfGI1eLKau1hNwqxGY1aTOBGRlJQUNbeeo9XkymK9h8XFxWpuNUy0Pod33nmnmovYTR1vu+02NV+2bJn5GDg2LVu2VHOrUZeIfRxZx/F9992n5jNmzFBzq9GV9RysY7QxGsX5bWhmNbIKBAJqbs0Tu3btUnPrPbYaeaHpWD9H/bLOA0RE+vXrp+aXXXaZmg8ZMkTNrZ+T1vH3ySefqPngwYPV3GokZzV5s5ruiogkJCSo+aJFi9R81apVan7uueeq+YABA9TcmgMao+mjdU5nnTdb55TZ2dn1HtORuJIBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAAp0LukzF9+nQ1v/fee9XcWjd5586dam71sBCx1x221l221j221la39m+tb2+tDW+t/V5eXq7mbdu2VXMR+zUsLCxU8/z8fDW31mW21nWurKxUc6tPhvUa3XzzzWouIrJt2zZzGzSNtLQ0NQ9lDX2rT4b1Gbf68aSmpqq5314zVh8Max6yXqNQ+mz47cVhzffWe7Rv3z41/+CDD9T80ksvVfPvv/9ezRujFwmObuDAgWpuvbfdunVTc2uOEQmtH48mJydHza2fg3576Vg/p60+NdZ5gnX8itjzYEZGhpqff/75am7NMX77rVjvkfX8rHMVEZHExERfj3HgwAE1t97nUDATAgAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwKmQF3M+ePCgmn/55Zdqbq09ba15bPVvEBGpqKhQc6sPhtWjISkpSc2jo6MbNLdYayJbvUhE7PWtU1JS1DwrK0vN4+Pj1dx6D6y1rWNjY9Xc6lGwYcMGNRcR+de//qXm1trT1md95cqV5hhwdH4/XyL2Z2j//v1qbr3/Vj8ca56yhIeHq7m1Rr7fPBTWXG09B+t9tNZ3X7NmjZrfeOONam7NtaG8h9ZcFspnFXVdffXVaj5o0CA1t/obuPj8W++9dS5gHR/W/q3naLFeA+s8IZS+Z5Y+ffqoud9zCatPhbV/6z0M5ZzW4rcfisVvzyYRrmQAAAAAcIwiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAqZD7ZHz88cdqbvXJWLdunZoPHjxYzf/rv/5LzUXsXhw9evRQc2tt9TZt2qi51UvE6sGwe/duNf/DH/6g5tnZ2Wo+ceJENRcRyc/PV/NVq1apudVDYMeOHWrevn17Nbc+Z+np6WpurRu9ZcsWNRexe21YeVlZmZpbvUbw4xISEnzvw1rffNu2bWpurX9u9VCw1sC39u937XS/4wtlG2uNemsuttZvt3qdWHOx9RpZz88av4j9PvntZXCisub4//mf/1HzkpISNc/MzFTzVq1aqbmI/TPAOpfo3Lmzmlt9KKx50OonlJiYqOYxMTFqbh0/ofSAaeh+P9YcYs1R1v2t51hVVeVr/6E8hrUPqxfI6tWr1fzcc89VcxGuZAAAAABwjCIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMCpkPtkWGvDjxs3Ts2t9XRfeuklNX///ffVHCIrV65Uc6uPh4jIvn371Pzrr79W8w4dOqi5tX63tb69tYa59fjW+uDWGuoi9vr2cXFxam6tQb5ixQpzDDg6q8fDnj17zH1Ya8Bv2rSpXmM6ktVDwVrf3W8fDGt9dqs/g/X6iNjHWWFhoZpbz9F6DZOSktR8165dan7GGWeo+SeffKLmocwjycnJap6bm2vu42RUXFys5lY/rNLSUjW3ekVZnx0Ruw+L9RyamtVzrFOnTmpu9W+w5mkRe57yyxqj1Y/Ib58Mqw+Hiz451nOw+mRYn9P//d//NcfAlQwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4FXKfDMv//d//qXlqaqqa/+QnP1HzCy+80ByDtTZ1z5491Xz48OFqPnPmTDVft26dmkdHR6u5ZcqUKWq+efNmNQ+l/4I1xssuu0zNy8rK1Lxjx45qfskll6j5P/7xDzVv06aNmn/55ZdqPnjwYDUXESkoKFBza/3s+fPnq/mnn35qjgFHZ/VPcLGPjRs3+tq/dYyFsoa8Jjw83Fdu9REIhdUPp6ioSM2t9d2tNeitx7fs379fza1eIQcOHDAfw+/7fLKyPjtffPGFmlv9CazjP5Sf49bnw+qlZI3B7zFu9UewPv85OTlqbv0MdMFvnwurn5C1fyu3WL2AXLDGWFFRoeYu3keuZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMCpkBfqtdaWtvLc3Fw1f/75533lIvba0KeddpqaP/fcc2qemJio5ikpKWpurZ1urets9cnYtm2bmgMnOqs/Qihrk1tr1G/ZsqVeYzqSNU9Za+xbvWis/UdFRam5tXa6NdeLiMTHx6t5cnKymlvr9FvvkdWPx/Lxxx+recuWLdV8586d5mO46OmC+rN6B1g9JKwcwH9wJQMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOhdwnw1pb2spbtPBXz1hrv4uIVFZWqvm6det85ScDv++T9Tnwe39rfFavEResPgHWGK3PstWnAD8uLi5OzUPp8WBtU1BQoOZJSUlqbvWp8DvXWv0Xqqqq1NxFnwyrj4TVC8RvL4Ju3br5uv8///lPNb/wwgvVPJR5MDY2tl5jAoDmhisZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcCrkPhl++e1f0Bj9D3D8v87Hw/isNfCrq6t95Th2kZGRam71kBARKSsr8zWGgwcPqvkXX3yh5lafDatPhfX5tHqJWH0yrOcnInLgwAE1j46OVvP27durudVr5ttvv1Vzy8KFC9V82LBhah5KvyH6ZAA40XElAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwqtGa8QFAQ0tMTFRzq1GdiEhubq6r4RyV1UiuVatWah4R4W/athoWJiQkqHlxcbH5GFbTTKspopVbzfz27t2r5pZNmzapeXl5uZpHRUWZj5Genl6vMQFAc8OVDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFnwwAJ4zU1FQ1j42NNfcRyjZ+vP/++77ub/VgsHpMWD0sqqqq1LxFC/t3U9Y2Vp8JS1lZmZr/7W9/87V/6zWorKxU85YtW5qP0aFDh3qNCQCaG65kAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCn6ZAA4YRw8eFDN4+LizH1UVFS4Gs5R/frXv27Q/aPh7d27V80TEhLMfezevdvVcADguMSVDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFnwwAJ4zKyko1Ly8vN/eRl5fnajhHFRYWpuae5zXo48O/goICNU9MTDT3YX1WAaC540oGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnArzWJQdAAAAgENcyQAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAntm3bJmFhYfLII4809VAANKKwsDCZMWNG8N/PPPOMhIWFybZt25psTGh6FBlNICwsLKT/Pvzww6YeKoDjzFdffSVXXXWVdOzYUaKjo6Vdu3YyfPhwmTVrVlMPDUAzcagIOPRfdHS0dOvWTaZPny45OTlNPTycICKaegAno+eff77Wv5977jlZtmxZndtPO+20xhwWgOPcZ599JkOGDJEOHTrI5MmTJSMjQ3bs2CErV66Uxx9/XG677bamHiKAZuS3v/2tdOrUScrKyuSTTz6ROXPmyKJFi2TdunUSGxvb1MNDM0eR0QSuv/76Wv9euXKlLFu2rM7tRyopKWmWB31xcbHExcU19TCAZu++++6TpKQk+fzzzyU5OblWtnfv3qYZVCNrrvMgcDy6+OKL5ZxzzhERkZtvvllSUlLksccekzfeeEPGjRvXxKNrOJyXNA6+LnWcGjx4sPTq1UtWrVolAwcOlNjYWPn1r38tIj+cTNx0002Snp4u0dHRcuaZZ8qzzz5b6/4ffvjhUb9ydeg7088880zwtj179sjEiROlffv2EhUVJW3atJGf/OQndb5LuXjxYrngggskLi5OEhISZOTIkfL111/X2mbChAkSHx8vmzdvlksuuUQSEhLkuuuuc/a6ACezzZs3S8+ePesUGCIiaWlpwf8PCwuT6dOny+uvvy69evWSqKgo6dmzpyxZsqTO/b7//nuZNGmSpKenB7d7+umna21TUVEhv/nNb6RPnz6SlJQkcXFxcsEFF8jy5cvNMXueJ1OmTJFAICALFiwI3v7CCy9Inz59JCYmRlq1aiVjx46VHTt21LqvNg8CcO/CCy8UEZGtW7fK4MGDZfDgwXW2mTBhgmRmZh7T/mfPni09e/aUqKgoadu2rUybNk3y8/OD+fTp0yU+Pl5KSkrq3HfcuHGSkZEh1dXVwds4Lzm+UWQcx/bv3y8XX3yx9O7dW2bOnClDhgyR0tJSGTx4sDz//PNy3XXXycMPPyxJSUkyYcIEefzxx4/pcUaPHi0LFy6UiRMnyuzZs+VnP/uZFBYWynfffRfc5vnnn5eRI0dKfHy8PPjgg3L33XfLN998IwMGDKhTjFRVVUl2drakpaXJI488IqNHj/bzMgD4/+vYsaOsWrVK1q1bZ277ySefyK233ipjx46Vhx56SMrKymT06NGyf//+4DY5OTnSv39/ee+992T69Ony+OOPS5cuXeSmm26SmTNnBrcrKCiQP//5zzJ48GB58MEHZcaMGZKbmyvZ2dmyZs2aHx1DdXW1TJgwQZ577jlZuHChXHnllSLywxWZG2+8Ubp27SqPPfaY3H777fL+++/LwIEDa51wiBx9HgTQMDZv3iwiIikpKc73PWPGDJk2bZq0bdtWHn30URk9erTMnTtXRowYIZWVlSIiMmbMGCkuLpZ33nmn1n1LSkrkrbfekquuukrCw8NFhPOSZsFDk5s2bZp35FsxaNAgT0S8p556qtbtM2fO9ETEe+GFF4K3VVRUeOedd54XHx/vFRQUeJ7necuXL/dExFu+fHmt+2/dutUTEW/+/Pme53leXl6eJyLeww8//KPjKyws9JKTk73JkyfXun3Pnj1eUlJSrdvHjx/viYj3y1/+MuTnDyA07777rhceHu6Fh4d75513nveLX/zCW7p0qVdRUVFrOxHxAoGAt2nTpuBta9eu9UTEmzVrVvC2m266yWvTpo23b9++WvcfO3asl5SU5JWUlHie53lVVVVeeXl5rW3y8vK89PR0b9KkScHbDs0vDz/8sFdZWemNGTPGi4mJ8ZYuXRrcZtu2bV54eLh333331drfV1995UVERNS6/cfmQQD+zJ8/3xMR77333vNyc3O9HTt2eC+++KKXkpLixcTEeDt37vQGDRrkDRo0qM59x48f73Xs2LHWbSLi3XPPPXX2v3XrVs/zPG/v3r1eIBDwRowY4VVXVwe3e/LJJz0R8Z5++mnP8zyvpqbGa9eunTd69Oha+3/55Zc9EfE++ugjz/M4L2kuuJJxHIuKipKJEyfWum3RokWSkZFR67uSkZGR8rOf/UyKiork73//e70eIyYmRgKBgHz44YeSl5d31G2WLVsm+fn5Mm7cONm3b1/wv/DwcOnXr99RvzJxyy231GscAGzDhw+XFStWyOWXXy5r166Vhx56SLKzs6Vdu3by5ptv1tp22LBhkpWVFfz3GWecIYmJibJlyxYR+eFrTK+99ppcdtll4nlerWM7OztbDh48KKtXrxYRkfDwcAkEAiIiUlNTIwcOHJCqqio555xzgtscrqKiQq6++mp5++23ZdGiRTJixIhgtmDBAqmpqZFrrrmm1mNmZGRI165d68wnR5sHAbgxbNgwSU1NlVNOOUXGjh0r8fHxsnDhQmnXrp3Tx3nvvfekoqJCbr/9dmnR4j+nnpMnT5bExMTglYuwsDC5+uqrZdGiRVJUVBTc7qWXXpJ27drJgAEDRITzkuaCP/w+jrVr1y74g/2Q7du3S9euXWsdpCL/WYlq+/bt9XqMqKgoefDBB+WOO+6Q9PR06d+/v1x66aVy4403SkZGhoiIbNy4UUT+813NIyUmJtb6d0REhLRv375e4wAQmr59+8qCBQukoqJC1q5dKwsXLpQ//vGPctVVV8maNWukR48eIiLSoUOHOvdt2bJl8JcJubm5kp+fL/PmzZN58+Yd9bEO/2PyZ599Vh599FFZv3598KsNIiKdOnWqc78HHnhAioqKZPHixXW+071x40bxPE+6du161MeMjIys9e+jzYMA3PjTn/4k3bp1k4iICElPT5dTTz21zvmFC4fOTU499dRatwcCAencuXOtc5cxY8bIzJkz5c0335Rrr71WioqKZNGiRTJ16lQJCwsTEc5LmguKjONYTEzMMd/30IF4pMP/YOqQ22+/XS677DJ5/fXXZenSpXL33XfLAw88IB988IGcddZZUlNTIyI/fP/xUOFxuIiI2h+jqKioBpmkAPxHIBCQvn37St++faVbt24yceJEeeWVV+See+4REQl+b/lInueJiASP6+uvv17Gjx9/1G3POOMMEfnhj7QnTJggo0aNkv/+7/+WtLQ0CQ8PlwceeCD4He7DZWdny5IlS+Shhx6SwYMHS3R0dDCrqamRsLAwWbx48VHHGB8fX+vffuZBALpzzz03uLrUkcLCwoLzxeGOdh7hUv/+/SUzM1Nefvllufbaa+Wtt96S0tJSGTNmTHAbzkuaB4qMZqZjx47y5ZdfSk1NTa0DZv369cFc5IffWIpInT+i/LErHVlZWXLHHXfIHXfcIRs3bpTevXvLo48+Ki+88ELwKxdpaWkybNgw108JgE+HThJ2794d8n1SU1MlISFBqqurzeP61Vdflc6dO8uCBQtq/QLjUEFzpP79+8tPf/pTufTSS+Xqq6+WhQsXBn/oZ2Vlied50qlTJ+nWrVvI4wXQuFq2bBn8euXh6vuNCZH/nJts2LBBOnfuHLy9oqJCtm7dWmcOuuaaa+Txxx+XgoICeemllyQzM1P69+8fzDkvaR4o65qZSy65RPbs2SMvvfRS8LaqqiqZNWuWxMfHy6BBg0TkhwM6PDxcPvroo1r3nz17dq1/l5SUSFlZWa3bsrKyJCEhQcrLy0Xkh99KJiYmyv3331/raxKH5ObmOnluAHTLly8/6m8WFy1aJCJ1v4qgCQ8Pl9GjR8trr7121NWqDj+uD11xOPyx//GPf8iKFSt+dP/Dhg2TF198UZYsWSI33HBD8DePV155pYSHh8u9995b57l4nldr9SsATScrK0vWr19fay5Yu3atfPrpp/Xe17BhwyQQCMgTTzxR67j/y1/+IgcPHpSRI0fW2n7MmDFSXl4uzz77rCxZskSuueaaWjnnJc0DVzKamSlTpsjcuXNlwoQJsmrVKsnMzJRXX31VPv30U5k5c6YkJCSIiEhSUpJcffXVMmvWLAkLC5OsrCx5++236zTs+vbbb2Xo0KFyzTXXSI8ePSQiIkIWLlwoOTk5MnbsWBH54buNc+bMkRtuuEHOPvtsGTt2rKSmpsp3330n77zzjpx//vny5JNPNvprAZxsbrvtNikpKZErrrhCunfvLhUVFfLZZ58Ff9NX3z+Q/sMf/iDLly+Xfv36yeTJk6VHjx5y4MABWb16tbz33nty4MABERG59NJLZcGCBXLFFVfIyJEjZevWrfLUU09Jjx49av1x5pFGjRol8+fPlxtvvFESExNl7ty5kpWVJb///e/lV7/6lWzbtk1GjRolCQkJsnXrVlm4cKFMmTJF7rzzTl+vEwD/Jk2aJI899phkZ2fLTTfdJHv37pWnnnpKevbsKQUFBfXaV2pqqvzqV7+Se++9Vy666CK5/PLLZcOGDTJ79mzp27dvnWbEZ599tnTp0kXuuusuKS8vr/VVKRHOS5qNJlrVCof5sSVse/bsedTtc3JyvIkTJ3qtW7f2AoGAd/rppweXpD1cbm6uN3r0aC82NtZr2bKlN3XqVG/dunW1lrDdt2+fN23aNK979+5eXFycl5SU5PXr1897+eWX6+xv+fLlXnZ2tpeUlORFR0d7WVlZ3oQJE7wvvvgiuM348eO9uLi4Y38xAPyoxYsXe5MmTfK6d+/uxcfHe4FAwOvSpYt32223eTk5OcHtRMSbNm1anft37NjRGz9+fK3bcnJyvGnTpnmnnHKKFxkZ6WVkZHhDhw715s2bF9ympqbGu//++72OHTt6UVFR3llnneW9/fbbdZayPHwJ28PNnj3bExHvzjvvDN722muveQMGDPDi4uK8uLg4r3v37t60adO8DRs2BLfR5kEAx+7QErOff/65ut0LL7zgde7c2QsEAl7v3r29pUuXHtMStoc8+eSTXvfu3b3IyEgvPT3du+WWW7y8vLyjPvZdd93liYjXpUuXHx0f5yXHtzDPO8q1dwAAAAA4RvxNBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJwKueN3WFhYQ47jhBAVFaXmK1asUPPt27ereWFhoZpHRkaq+ddff63mIiLnnnuumh88eFDNa2pq1Lx9+/ZqfsUVV6h5fbuMnoiac2sb5hHg+NBc5xHmEJEWLfz9ftj6OW25++671fzMM89U88rKSvMxysvL1XzChAnmPjTh4eFqbh0ffl/DE0EocwhXMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnAp5CVvYTj31VDU/66yz1DwmJkbNy8rK1DwxMVHNO3TooOYiIi1btlTznJwcNY+OjlbzjIwMNT/jjDPU/JNPPlFzAAAaSihL6FpLe1pL0FrLozb08qmBQEDNf/vb36r5V199pealpaXmGKwlZjMzM9V827Ztal5dXW2OQcMSuKHhSgYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcok+GQ506dVJzq8/Fnj171Nxal7mkpETNrT4cIiK5ublqbq2fXVFRoebW2tFJSUlqDgBAU7F+hon474NhsXp1jBgxQs2vu+46NR8yZIiar169Ws2tc52qqio1F7H7WLz55ptqvnLlSjVfsGCBmi9ZskTN/fbZaOjPyPGCKxkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwij4ZDp1//vlqnpeXp+ZWHwxrfW5r7emCggI1F7HXZk5ISFDz4uJiNS8qKlLzbt26qfk777yj5gAANBSrv4GI/XPU6nMxd+5cNT/nnHPU3Po5bZ1LWD27rJ5cVp+MUHpMWNtY5zu9e/dW8z59+qj5nXfeqea7du1S8wkTJqi59RmxzgdF/PfqaAxcyQAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBT9Mlw6Oyzz1Zzq0+GJS4uztf9rXWlRUQqKirU3Fpf22K9Bp07d/a1fwAAGorV3yAUf/rTn9R80KBBar537141LywsVHPr57jVo8HqFVJaWqrmLvo7WO+D1ZPLeg5WL5MuXbqo+axZs9R82rRpat4cemCEgisZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTNONzKCEhQc2tBjUHDx5Uc6s5S2Zmppq3bt1azUVECgoK1NxqAmQ1+bFeg8TERDUH0LSsJlUi9jwwatQoNd+3b5+af/LJJ+YYGpL1GoTyGllcNH1D08jKylJzq3Hv9u3b1TwqKkrNrUZz1vFpnWtYeWVlpa/HF/H/+Y+MjFRz6xi1Hn/btm1q3r17dzUfOXKkmr/zzjtq3lxwJQMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABO0SfDofz8fDVv1aqVmpeXl/t6/JycHDX/+c9/bu7j5ZdfVnOrT4b1HKz1s0tKStQcQNMKZY17S1JSkpq3a9dOzQ8cOKDm33zzTb3HVB/Wa+DiNULz1a1bNzUPBAJqHh0dreZlZWVqbvXJsHKrR4TV76qiokLNrT4foWxjPYbfPhjFxcVqHh4erubWe9y3b181p08GAAAAABwFRQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFP0yXBo586dat62bVs1t9Z9ttZlTkxMVPM333xTzUXsPhUxMTFqbq1NbT0Hq9cIAJ11DDZGD4dRo0ap+ZQpU9TcWod/5MiRam7NM99++62ab9iwQc3fffddX/vHia1///5qbvWpsPpkHDx40Nf9rX5VVg8JK3cxB1ljrKqqUvOioiI1t8Zo3b9NmzZqbr3HLVu2VPMTBVcyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BR9Mhzatm2bmg8aNEjNq6ur1TwiQn+7rHWZQ1FQUKDmCQkJam6tn22tX5+Xl6fmAHSN0QfDMmLECDWPjY1Vc6tnkNWvx5pLe/bsqea9evVS88svv1zNIyMj1VxEZPfu3Wpu9Qx666231HzXrl3mGNAwunXrpuZWH5ioqCg1t3piWT0k/PagsHLrXCSU48PvGCzWuYo1h1hzUHFxsZqnp6er+YmCKxkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwij4ZDlnrnltrX1trV8fHx6v5/v371TwUO3bsUPPTTz9dzcvLy9Xc6rNhrQ0PoGmFssZ9ly5d1Dw3N1fNrZ5AVi+QsLAwX/e3BAIBNQ9lDf+UlBQ1b9mypZo/+eSTav7cc8+ZY0DDaN++vZr77ZNhnQtY/aasz6/V58I6vvzmIvYcYPWpsI5x6xi1evVY86DVJ8M6vk8UXMkAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAU/TJcMha+91SU1Oj5snJyWr+ySef+Hp8EZEPP/xQzc877zw1t9a2ttb/3rt3r5oDaFqtWrUyt7HWkLf64Vj9dKyeQhZrrq2urlZza56z+hCEwlqnf/PmzWp+7733qvmMGTPqOySEKC4uTs2tnlbW59M6Pvbs2aPmfvvQWKw+G9bxFcoYrOdgPYbVq8TqRWK9BwcPHlRz63zuRMGVDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFnwyHcnJy1NzqEZGUlKTm4eHham6tvR2Kb7/9Vs137Njha//Wc7DWlgaau7CwMDX3u0Z9Q7v88svNbdq0aaPm1jr6BQUF9RrTkaw+FVYfAr99LqweFyIiRUVFap6WlqbmmzZtqteY0HisHgt+f1ZbPRb8zjHW8ek3r6qqUvNQtrGOYb/P0ZoDrD4d1ntgfUZOFFzJAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFP0yXDo3//+t5pXVlaqubVus8XafyisPhXV1dW+9m+tPZ2bm+tr/4BfDd3HwuoV43f/fo/RP/7xj2qemZlp7uONN95Q8/bt26t5QkKCmkdHR6u5tYa+9R5b9/f7GovY87X1GHl5eb7HgPqzPnsi9s856xi3ejC0atVKza3PjjUHWceHdX/r+HHB7zzasmVLNV+xYoWap6SkqHlMTIyaW6znJ+JmHmpoXMkAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAU/TJcMjqMVFaWqrm1trY1rrJ+/fvV/NQFBYWqrm1/rW1NrWVb9y4Uc0BP6z1348HDb32+R133KHmp512mpqvXbvWfIy2bduqeXx8vJrHxcWpudVTyJpnqqqq1NwSGRmp5qG8h9Z8bv08+Pzzz83HgHtWfwUR/8ewdf+Kigo1b+hePH7n0VB6glnHmN9zEWsOWrdunZoPHDhQza1eP7t27VJzqxeKSPPoK8aVDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFn4xGZK19ba09vXPnTjV3sWaytf623/W1rV4e5eXlvvYPaPx+fkXs47ShezS0a9dOze+55x41t9b5/+c//6nmvXr1UnMRkcTERDW31vG3XiOrh4Q111rvkd/7h9JHIDo6Ws0DgYCaW3M1Gob12Rax33+/c8if//xnNR8yZIiaf//992ruVyh9MCx+z5cs1v2Li4vV3Dp+rffQeo1SUlLUXIQ+GQAAAABOQhQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIpmfI3IajDlt4FPWVlZvcd0pJycHDW3GsxYTbb8NqipqalRczRv1vvf0A2YROzj1G9Dvx49eqj573//ezU/ePCgmn/33Xe+7t++fXs1j4qKUnMR+zi1GslZja6s98jKS0tL1dzSqlUrX48fyjbWa2g1ZUTDiI+PN7fx2+wuISFBzf/0pz+p+fDhw9Xc7zx6PPDb8M86V6msrFTzPXv2qHlmZqaaWz9HYmNj1by54EoGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKJPRiOy1qa21n221q/fvHlzvcd0pE2bNql5SkqKmu/cudPX47ds2VLN9+/f72v/OL6dCH1QUlNT1fx//ud/1HzNmjVqHhkZqeaJiYlqnpGRoebJyclqHhcXp+YiofWJ0Fh9LMrLy9Xcmmutecy6f2FhoZq76FlUUlKi5unp6b4fA/UXExPT4I9h9bLZsWOHmgcCATW3ejREROinhn7n6VD6dPgdgzVPWv1+rPc5Pz9fza3naL0HjfE5awxcyQAAAADgFEUGAAAAAKcoMgAAAAA4RZEBAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBT9MlwyFqXOSEhQc3Dw8PVvLKyUs3Xr1+v5i6sWLFCza01+K3n0LNnTzX/6KOP1BzNm/X+n3LKKWqem5ur5vv27TPH0KVLFzUfOnSomls9GPLy8tTcWh/eeg2sPhbW+Kx5yOpRISJSXV2t5tYa8tZcaq2RHx8fr+ZWDwqrT4fFGn8orHX0rX4saBgu+hdYn489e/aoeXR0tJpbc4DVx8Zvnw0rd8GaA6znaM2zFquXidX3zGK9B80FVzIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFH0yHLLWn7fW6I+NjVVzq89GY7DW7+7WrZuv+7dr167eY0LzMWDAADW/4oor1Hznzp1qftppp6l5enq6movYa7yvXbtWzXv06KHmycnJap6VlaXmiYmJau63x4S1vnwofTKioqLMbfyw1qC3+ghYfTCsPh7WXG31AxKxe4lY+0hKSjIfA+6F0ifDmkOsY3T16tVqbvVQsMZozQFWrxzr/tbx44L1Glrvgd9eIHv37lVza46yXkP6ZAAAAADAUVBkAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZ8Mh/Lz89XcWnc5IkJ/O6z16xuDtTa1tbazdX9rfW40b1u3blXz9evXq3lGRoaaW+uzh9LjwToOrV4uVo+Ili1bqrm1vrr1HKzHt44xK7fGJyJSUVGh5laPCGuutNaYLygoUHPrc+K3z4f1ORURKS4uVnPrNbQ+R2gYVp8aEfvzaf0cXLRokZpbc5Tfz7c1fmsOsB4/lDnE4rdXh5Vbr/GuXbvU3HqPrdfA6sXTXHAlAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE7RJ8OhsrIyX7m1Pv3mzZvrPSbXCgsL1dxa+9lae9paOx7Nm7X2/yuvvKLmaWlpat6pUyc179Gjh5qL2Ovg+10/PS8vT82tY2Dfvn1qbq2vbvVXOOWUU9Q8MzNTzUMZg98+GdYa9Nb+rXnKmovj4uLU3Or3IiKybNkyNS8tLVVz6zmiYVifTRH789W+fXs1/+c//6nmvXr1UnPrXMPqMWE9R7/9rKzHD2UMDd2LIyEhQc1XrVql5tb4rOdnzTHNBVcyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BR9MhpR27Zt1Xz//v1qHsr63A3NGqO1NrTftaPRvFnroycnJ6t5eXm5mq9evVrNv/jiCzUXsXs8WOuXt27dWs2t/gdt2rRR8z/+8Y9qvm3bNjW3+nxY68uHsv68tY21Bn10dLSa+12n31JVVaXmGRkZav7iiy+aj/HMM8+oudUTplWrVmpuvc84NqF89qyfc3517NhRza1+RH57SFjHh/X8Q+mTYfF7rlBZWanmVi8Tq6dTly5d1Nzqp0SfDAAAAAA4CooMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnWEi7EZWVlal5Q6/97sLevXvV3Fp/21o/OyYmpt5jQvNh9Vmxcqu/gpXHx8eruYhIIBBQc2uN+J07d5qPoVm1apWaP/DAA2p+xhlnqHleXp6aW/0VsrKy1FxEpLq6Ws2tecB6ja1+KdYa+F9//bWaW+v4W3P5t99+q+Yidq+NDh06qLk110ZFRZljQP25+DltfT4t1jFozaN+f05bGqPflTVG632y5sEePXrUe0yHO3DggJpb8+yJci7ElQwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFEUGAAAAAKcoMgAAAAA4RZ+MRrR161Y1b9OmTSON5NitX79eza315f2uv43mzXr/rfXVCwsLfeWhsMZo9dqw+my0bNnS1+NPnjxZzYcOHarmGzZsUHNrjf2SkhI1FxHJyclRc6tPgHV/q49Gc2B9TvLz89Xceg2tXiU4NtHR0eY21ntrHYOW4cOHq7k1x1ifjcjIyHqP6XDWPB7KZ9NvPxK/zzEzM9PX42/ZskXNrc/RiXKuxJUMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACcosgAAAAA4BRFBgAAAACnKDIAAAAAOEWRAQAAAMApmvE1Iqu5jNXIrkWLpq8Jd+/e7ev+x8NzQNOxmjQdD/w2BLRYze4sK1eu9JXj+FBRUdHUQ8AxiI2NNbexGqn9+9//9jWGm2++Wc179Oih5lZD0bS0NDW3mg1ajeYiIuxTT7/nClbT0LKyMjXfsWOHr8ffvHmzmp966qm+9t9ccMYHAAAAwCmKDAAAAABOUWQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAAp+iT0YisdZujoqLU/HhYV724uFjNq6ur1TwyMlLNS0tL6z0mAAAaQ+vWrX3vIz8/39f9c3JyfOVoeLm5uWpu9TI5Hs73XOBKBgAAAACnKDIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiT0YjiouLU/P4+Hg1t9Zdbgw7d+5U84MHD6p5dHS0mgcCgXqPCQCAxpCWlmZuk5KSouZJSUm+xtCihf77Yb+5paqqSs09z/O1fxescwnrOdTU1Ki59RzT09PVPDk5Wc27du2q5s0FVzIAAAAAOEWRAQAAAMApigwAAAAATlFkAAAAAHCKIgMAAACAUxQZAAAAAJyiyAAAAADgFH0yGtGHH36o5gkJCWq+YsUKh6M5Ntba0Dk5OWrepk0bNV+zZk19hwQAQKNYvny5uY3VS+Pjjz/2NQbr57DVA+JkUF5e3qSP//3336t5WFiYmlt9OpoLrmQAAAAAcIoiAwAAAIBTFBkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAqTDPWnAZAAAAAOqBKxkAAAAAnKLIAAAAAOAURQYAAAAApygyAAAAADhFkQEAAADAKYoMAAAAAE5RZAAAAABwiiIDAAAAgFMUGQAAAACc+v8B4FgluNSSlewAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten= nn.Flatten()\n",
        "    self.fc1=nn.Linear(in_features=784,out_features=128)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.fc2=nn.Linear(in_features=128,out_features=10)\n",
        "  def forward(self,x):\n",
        "    x=self.flatten(x)\n",
        "    x=self.fc1(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.fc2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "RgG3piFPloCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=FashionClassifier()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYrbEz6tmurt",
        "outputId": "0842f887-99c8-426c-f63a-82e79beb184e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FashionClassifier(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(params=model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "w-mg8OIWnj34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "  print(f\"epoch{epoch+1}/{epochs}\")\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch, (X, y) in enumerate(train_loader):\n",
        "        y_pred = model(X)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0:\n",
        "            print(f\"  Batch {batch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "  print(f\"Epoch {epoch+1} completed with average loss: {train_loss/len(train_loader):.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94l6WkL7oHpb",
        "outputId": "2ef34cb8-3aaf-4512-a2e2-a6413ccbd218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch1/100\n",
            "  Batch 0, Loss: 2.3082\n",
            "  Batch 100, Loss: 1.8816\n",
            "  Batch 200, Loss: 1.4093\n",
            "  Batch 300, Loss: 1.1719\n",
            "  Batch 400, Loss: 1.0726\n",
            "  Batch 500, Loss: 0.9758\n",
            "  Batch 600, Loss: 0.9086\n",
            "  Batch 700, Loss: 0.8617\n",
            "  Batch 800, Loss: 0.6900\n",
            "  Batch 900, Loss: 1.0353\n",
            "  Batch 1000, Loss: 0.8511\n",
            "  Batch 1100, Loss: 0.6233\n",
            "  Batch 1200, Loss: 0.6179\n",
            "  Batch 1300, Loss: 0.6858\n",
            "  Batch 1400, Loss: 0.6204\n",
            "  Batch 1500, Loss: 0.7373\n",
            "  Batch 1600, Loss: 0.6414\n",
            "  Batch 1700, Loss: 0.4605\n",
            "  Batch 1800, Loss: 0.4442\n",
            "Epoch 1 completed with average loss: 0.9115\n",
            "\n",
            "epoch2/100\n",
            "  Batch 0, Loss: 0.5114\n",
            "  Batch 100, Loss: 0.5641\n",
            "  Batch 200, Loss: 0.6117\n",
            "  Batch 300, Loss: 0.4540\n",
            "  Batch 400, Loss: 0.5274\n",
            "  Batch 500, Loss: 0.4030\n",
            "  Batch 600, Loss: 0.6494\n",
            "  Batch 700, Loss: 0.4137\n",
            "  Batch 800, Loss: 0.4879\n",
            "  Batch 900, Loss: 0.5797\n",
            "  Batch 1000, Loss: 0.4024\n",
            "  Batch 1100, Loss: 0.5361\n",
            "  Batch 1200, Loss: 0.5573\n",
            "  Batch 1300, Loss: 0.6126\n",
            "  Batch 1400, Loss: 0.5246\n",
            "  Batch 1500, Loss: 0.5947\n",
            "  Batch 1600, Loss: 0.5651\n",
            "  Batch 1700, Loss: 0.5576\n",
            "  Batch 1800, Loss: 0.5039\n",
            "Epoch 2 completed with average loss: 0.5593\n",
            "\n",
            "epoch3/100\n",
            "  Batch 0, Loss: 0.8099\n",
            "  Batch 100, Loss: 0.3549\n",
            "  Batch 200, Loss: 0.4442\n",
            "  Batch 300, Loss: 0.3999\n",
            "  Batch 400, Loss: 0.5253\n",
            "  Batch 500, Loss: 0.2455\n",
            "  Batch 600, Loss: 0.4679\n",
            "  Batch 700, Loss: 0.5166\n",
            "  Batch 800, Loss: 0.4670\n",
            "  Batch 900, Loss: 0.3597\n",
            "  Batch 1000, Loss: 0.2865\n",
            "  Batch 1100, Loss: 0.4846\n",
            "  Batch 1200, Loss: 0.3248\n",
            "  Batch 1300, Loss: 0.4537\n",
            "  Batch 1400, Loss: 0.2960\n",
            "  Batch 1500, Loss: 0.4716\n",
            "  Batch 1600, Loss: 0.4030\n",
            "  Batch 1700, Loss: 0.6305\n",
            "  Batch 1800, Loss: 0.3879\n",
            "Epoch 3 completed with average loss: 0.4990\n",
            "\n",
            "epoch4/100\n",
            "  Batch 0, Loss: 0.5823\n",
            "  Batch 100, Loss: 0.2727\n",
            "  Batch 200, Loss: 0.3500\n",
            "  Batch 300, Loss: 0.6721\n",
            "  Batch 400, Loss: 0.3020\n",
            "  Batch 500, Loss: 0.4168\n",
            "  Batch 600, Loss: 0.4717\n",
            "  Batch 700, Loss: 0.3511\n",
            "  Batch 800, Loss: 0.1197\n",
            "  Batch 900, Loss: 0.6156\n",
            "  Batch 1000, Loss: 0.3830\n",
            "  Batch 1100, Loss: 0.3616\n",
            "  Batch 1200, Loss: 0.7448\n",
            "  Batch 1300, Loss: 0.5136\n",
            "  Batch 1400, Loss: 0.3881\n",
            "  Batch 1500, Loss: 0.4159\n",
            "  Batch 1600, Loss: 0.3546\n",
            "  Batch 1700, Loss: 0.3564\n",
            "  Batch 1800, Loss: 0.4229\n",
            "Epoch 4 completed with average loss: 0.4690\n",
            "\n",
            "epoch5/100\n",
            "  Batch 0, Loss: 0.3162\n",
            "  Batch 100, Loss: 0.6439\n",
            "  Batch 200, Loss: 0.3149\n",
            "  Batch 300, Loss: 0.3203\n",
            "  Batch 400, Loss: 0.6699\n",
            "  Batch 500, Loss: 0.5588\n",
            "  Batch 600, Loss: 0.7553\n",
            "  Batch 700, Loss: 0.5185\n",
            "  Batch 800, Loss: 0.3302\n",
            "  Batch 900, Loss: 0.4394\n",
            "  Batch 1000, Loss: 0.7822\n",
            "  Batch 1100, Loss: 0.6380\n",
            "  Batch 1200, Loss: 0.4799\n",
            "  Batch 1300, Loss: 0.5299\n",
            "  Batch 1400, Loss: 0.6040\n",
            "  Batch 1500, Loss: 0.2965\n",
            "  Batch 1600, Loss: 0.3871\n",
            "  Batch 1700, Loss: 0.4691\n",
            "  Batch 1800, Loss: 0.4859\n",
            "Epoch 5 completed with average loss: 0.4502\n",
            "\n",
            "epoch6/100\n",
            "  Batch 0, Loss: 0.2611\n",
            "  Batch 100, Loss: 0.2416\n",
            "  Batch 200, Loss: 0.2554\n",
            "  Batch 300, Loss: 0.2766\n",
            "  Batch 400, Loss: 0.4041\n",
            "  Batch 500, Loss: 0.3878\n",
            "  Batch 600, Loss: 0.1415\n",
            "  Batch 700, Loss: 0.4637\n",
            "  Batch 800, Loss: 0.3121\n",
            "  Batch 900, Loss: 0.4501\n",
            "  Batch 1000, Loss: 0.2080\n",
            "  Batch 1100, Loss: 0.4571\n",
            "  Batch 1200, Loss: 0.5437\n",
            "  Batch 1300, Loss: 0.2808\n",
            "  Batch 1400, Loss: 0.6663\n",
            "  Batch 1500, Loss: 0.2788\n",
            "  Batch 1600, Loss: 0.5359\n",
            "  Batch 1700, Loss: 0.2690\n",
            "  Batch 1800, Loss: 0.6294\n",
            "Epoch 6 completed with average loss: 0.4357\n",
            "\n",
            "epoch7/100\n",
            "  Batch 0, Loss: 0.4934\n",
            "  Batch 100, Loss: 0.3081\n",
            "  Batch 200, Loss: 0.5290\n",
            "  Batch 300, Loss: 0.6922\n",
            "  Batch 400, Loss: 0.3043\n",
            "  Batch 500, Loss: 0.6770\n",
            "  Batch 600, Loss: 0.3227\n",
            "  Batch 700, Loss: 0.4705\n",
            "  Batch 800, Loss: 0.3211\n",
            "  Batch 900, Loss: 0.3145\n",
            "  Batch 1000, Loss: 0.5662\n",
            "  Batch 1100, Loss: 0.4012\n",
            "  Batch 1200, Loss: 0.3358\n",
            "  Batch 1300, Loss: 0.3831\n",
            "  Batch 1400, Loss: 0.5123\n",
            "  Batch 1500, Loss: 0.2926\n",
            "  Batch 1600, Loss: 0.3306\n",
            "  Batch 1700, Loss: 0.4246\n",
            "  Batch 1800, Loss: 0.2549\n",
            "Epoch 7 completed with average loss: 0.4228\n",
            "\n",
            "epoch8/100\n",
            "  Batch 0, Loss: 0.4681\n",
            "  Batch 100, Loss: 0.2960\n",
            "  Batch 200, Loss: 0.5522\n",
            "  Batch 300, Loss: 0.2675\n",
            "  Batch 400, Loss: 0.2769\n",
            "  Batch 500, Loss: 0.4255\n",
            "  Batch 600, Loss: 0.4664\n",
            "  Batch 700, Loss: 0.1921\n",
            "  Batch 800, Loss: 0.3829\n",
            "  Batch 900, Loss: 0.3533\n",
            "  Batch 1000, Loss: 0.4191\n",
            "  Batch 1100, Loss: 0.4342\n",
            "  Batch 1200, Loss: 0.4867\n",
            "  Batch 1300, Loss: 0.2318\n",
            "  Batch 1400, Loss: 0.2763\n",
            "  Batch 1500, Loss: 0.4243\n",
            "  Batch 1600, Loss: 0.6374\n",
            "  Batch 1700, Loss: 0.2500\n",
            "  Batch 1800, Loss: 0.7218\n",
            "Epoch 8 completed with average loss: 0.4121\n",
            "\n",
            "epoch9/100\n",
            "  Batch 0, Loss: 0.3670\n",
            "  Batch 100, Loss: 0.3322\n",
            "  Batch 200, Loss: 0.3733\n",
            "  Batch 300, Loss: 0.4844\n",
            "  Batch 400, Loss: 0.5354\n",
            "  Batch 500, Loss: 0.2927\n",
            "  Batch 600, Loss: 0.3570\n",
            "  Batch 700, Loss: 0.4253\n",
            "  Batch 800, Loss: 0.2111\n",
            "  Batch 900, Loss: 0.5481\n",
            "  Batch 1000, Loss: 0.6582\n",
            "  Batch 1100, Loss: 0.3362\n",
            "  Batch 1200, Loss: 0.3231\n",
            "  Batch 1300, Loss: 0.3785\n",
            "  Batch 1400, Loss: 0.3554\n",
            "  Batch 1500, Loss: 0.2921\n",
            "  Batch 1600, Loss: 0.3153\n",
            "  Batch 1700, Loss: 0.4504\n",
            "  Batch 1800, Loss: 0.4032\n",
            "Epoch 9 completed with average loss: 0.4034\n",
            "\n",
            "epoch10/100\n",
            "  Batch 0, Loss: 0.3924\n",
            "  Batch 100, Loss: 0.5836\n",
            "  Batch 200, Loss: 0.3861\n",
            "  Batch 300, Loss: 0.5063\n",
            "  Batch 400, Loss: 0.2640\n",
            "  Batch 500, Loss: 0.2823\n",
            "  Batch 600, Loss: 0.5060\n",
            "  Batch 700, Loss: 0.5079\n",
            "  Batch 800, Loss: 0.3522\n",
            "  Batch 900, Loss: 0.4178\n",
            "  Batch 1000, Loss: 0.4756\n",
            "  Batch 1100, Loss: 0.2793\n",
            "  Batch 1200, Loss: 0.3976\n",
            "  Batch 1300, Loss: 0.5144\n",
            "  Batch 1400, Loss: 0.3893\n",
            "  Batch 1500, Loss: 0.4201\n",
            "  Batch 1600, Loss: 0.4437\n",
            "  Batch 1700, Loss: 0.3801\n",
            "  Batch 1800, Loss: 0.2260\n",
            "Epoch 10 completed with average loss: 0.3941\n",
            "\n",
            "epoch11/100\n",
            "  Batch 0, Loss: 0.1392\n",
            "  Batch 100, Loss: 0.2916\n",
            "  Batch 200, Loss: 0.5822\n",
            "  Batch 300, Loss: 0.2287\n",
            "  Batch 400, Loss: 0.3199\n",
            "  Batch 500, Loss: 0.6110\n",
            "  Batch 600, Loss: 0.3526\n",
            "  Batch 700, Loss: 0.4469\n",
            "  Batch 800, Loss: 0.3311\n",
            "  Batch 900, Loss: 0.6046\n",
            "  Batch 1000, Loss: 0.4588\n",
            "  Batch 1100, Loss: 0.4651\n",
            "  Batch 1200, Loss: 0.3039\n",
            "  Batch 1300, Loss: 0.5961\n",
            "  Batch 1400, Loss: 0.3833\n",
            "  Batch 1500, Loss: 0.2114\n",
            "  Batch 1600, Loss: 0.3772\n",
            "  Batch 1700, Loss: 0.4570\n",
            "  Batch 1800, Loss: 0.3551\n",
            "Epoch 11 completed with average loss: 0.3862\n",
            "\n",
            "epoch12/100\n",
            "  Batch 0, Loss: 0.2758\n",
            "  Batch 100, Loss: 0.1756\n",
            "  Batch 200, Loss: 0.3803\n",
            "  Batch 300, Loss: 0.2911\n",
            "  Batch 400, Loss: 0.3768\n",
            "  Batch 500, Loss: 0.5347\n",
            "  Batch 600, Loss: 0.2744\n",
            "  Batch 700, Loss: 0.2632\n",
            "  Batch 800, Loss: 0.4565\n",
            "  Batch 900, Loss: 0.3749\n",
            "  Batch 1000, Loss: 0.4741\n",
            "  Batch 1100, Loss: 0.7640\n",
            "  Batch 1200, Loss: 0.4348\n",
            "  Batch 1300, Loss: 0.2580\n",
            "  Batch 1400, Loss: 0.3277\n",
            "  Batch 1500, Loss: 0.5589\n",
            "  Batch 1600, Loss: 0.4495\n",
            "  Batch 1700, Loss: 0.4106\n",
            "  Batch 1800, Loss: 0.5505\n",
            "Epoch 12 completed with average loss: 0.3796\n",
            "\n",
            "epoch13/100\n",
            "  Batch 0, Loss: 0.2628\n",
            "  Batch 100, Loss: 0.1964\n",
            "  Batch 200, Loss: 0.2651\n",
            "  Batch 300, Loss: 0.4103\n",
            "  Batch 400, Loss: 0.3903\n",
            "  Batch 500, Loss: 0.3459\n",
            "  Batch 600, Loss: 0.5672\n",
            "  Batch 700, Loss: 0.2113\n",
            "  Batch 800, Loss: 0.4514\n",
            "  Batch 900, Loss: 0.2628\n",
            "  Batch 1000, Loss: 0.5330\n",
            "  Batch 1100, Loss: 0.4225\n",
            "  Batch 1200, Loss: 0.2878\n",
            "  Batch 1300, Loss: 0.3188\n",
            "  Batch 1400, Loss: 0.4391\n",
            "  Batch 1500, Loss: 0.3350\n",
            "  Batch 1600, Loss: 0.4243\n",
            "  Batch 1700, Loss: 0.1588\n",
            "  Batch 1800, Loss: 0.3041\n",
            "Epoch 13 completed with average loss: 0.3722\n",
            "\n",
            "epoch14/100\n",
            "  Batch 0, Loss: 0.5140\n",
            "  Batch 100, Loss: 0.3080\n",
            "  Batch 200, Loss: 0.3736\n",
            "  Batch 300, Loss: 0.1168\n",
            "  Batch 400, Loss: 0.4418\n",
            "  Batch 500, Loss: 0.3954\n",
            "  Batch 600, Loss: 0.5090\n",
            "  Batch 700, Loss: 0.3847\n",
            "  Batch 800, Loss: 0.3906\n",
            "  Batch 900, Loss: 0.1837\n",
            "  Batch 1000, Loss: 0.5145\n",
            "  Batch 1100, Loss: 0.1333\n",
            "  Batch 1200, Loss: 0.3518\n",
            "  Batch 1300, Loss: 0.5450\n",
            "  Batch 1400, Loss: 0.2971\n",
            "  Batch 1500, Loss: 0.1838\n",
            "  Batch 1600, Loss: 0.3419\n",
            "  Batch 1700, Loss: 0.3540\n",
            "  Batch 1800, Loss: 0.2122\n",
            "Epoch 14 completed with average loss: 0.3658\n",
            "\n",
            "epoch15/100\n",
            "  Batch 0, Loss: 0.2330\n",
            "  Batch 100, Loss: 0.1735\n",
            "  Batch 200, Loss: 0.2224\n",
            "  Batch 300, Loss: 0.5143\n",
            "  Batch 400, Loss: 0.5214\n",
            "  Batch 500, Loss: 0.4017\n",
            "  Batch 600, Loss: 0.3224\n",
            "  Batch 700, Loss: 0.2707\n",
            "  Batch 800, Loss: 0.2672\n",
            "  Batch 900, Loss: 0.2102\n",
            "  Batch 1000, Loss: 0.2744\n",
            "  Batch 1100, Loss: 0.2056\n",
            "  Batch 1200, Loss: 0.4010\n",
            "  Batch 1300, Loss: 0.3312\n",
            "  Batch 1400, Loss: 0.3407\n",
            "  Batch 1500, Loss: 0.5830\n",
            "  Batch 1600, Loss: 0.3395\n",
            "  Batch 1700, Loss: 0.2006\n",
            "  Batch 1800, Loss: 0.3072\n",
            "Epoch 15 completed with average loss: 0.3600\n",
            "\n",
            "epoch16/100\n",
            "  Batch 0, Loss: 0.1426\n",
            "  Batch 100, Loss: 0.2392\n",
            "  Batch 200, Loss: 0.2476\n",
            "  Batch 300, Loss: 0.4034\n",
            "  Batch 400, Loss: 0.4337\n",
            "  Batch 500, Loss: 0.1528\n",
            "  Batch 600, Loss: 0.4078\n",
            "  Batch 700, Loss: 0.3388\n",
            "  Batch 800, Loss: 0.2848\n",
            "  Batch 900, Loss: 0.4542\n",
            "  Batch 1000, Loss: 0.3144\n",
            "  Batch 1100, Loss: 0.3909\n",
            "  Batch 1200, Loss: 0.5071\n",
            "  Batch 1300, Loss: 0.2334\n",
            "  Batch 1400, Loss: 0.6380\n",
            "  Batch 1500, Loss: 0.3489\n",
            "  Batch 1600, Loss: 0.5548\n",
            "  Batch 1700, Loss: 0.2325\n",
            "  Batch 1800, Loss: 0.3341\n",
            "Epoch 16 completed with average loss: 0.3548\n",
            "\n",
            "epoch17/100\n",
            "  Batch 0, Loss: 0.5446\n",
            "  Batch 100, Loss: 0.2168\n",
            "  Batch 200, Loss: 0.4069\n",
            "  Batch 300, Loss: 0.2504\n",
            "  Batch 400, Loss: 0.2358\n",
            "  Batch 500, Loss: 0.2186\n",
            "  Batch 600, Loss: 0.1761\n",
            "  Batch 700, Loss: 0.5885\n",
            "  Batch 800, Loss: 0.4378\n",
            "  Batch 900, Loss: 0.4148\n",
            "  Batch 1000, Loss: 0.1633\n",
            "  Batch 1100, Loss: 0.3791\n",
            "  Batch 1200, Loss: 0.3735\n",
            "  Batch 1300, Loss: 0.3100\n",
            "  Batch 1400, Loss: 0.3456\n",
            "  Batch 1500, Loss: 0.3673\n",
            "  Batch 1600, Loss: 0.2173\n",
            "  Batch 1700, Loss: 0.4236\n",
            "  Batch 1800, Loss: 0.5761\n",
            "Epoch 17 completed with average loss: 0.3502\n",
            "\n",
            "epoch18/100\n",
            "  Batch 0, Loss: 0.2863\n",
            "  Batch 100, Loss: 0.4679\n",
            "  Batch 200, Loss: 0.1504\n",
            "  Batch 300, Loss: 0.2857\n",
            "  Batch 400, Loss: 0.7361\n",
            "  Batch 500, Loss: 0.3441\n",
            "  Batch 600, Loss: 0.3775\n",
            "  Batch 700, Loss: 0.3612\n",
            "  Batch 800, Loss: 0.3567\n",
            "  Batch 900, Loss: 0.3926\n",
            "  Batch 1000, Loss: 0.2235\n",
            "  Batch 1100, Loss: 0.1389\n",
            "  Batch 1200, Loss: 0.2922\n",
            "  Batch 1300, Loss: 0.4606\n",
            "  Batch 1400, Loss: 0.4593\n",
            "  Batch 1500, Loss: 0.2779\n",
            "  Batch 1600, Loss: 0.4903\n",
            "  Batch 1700, Loss: 0.1783\n",
            "  Batch 1800, Loss: 0.2316\n",
            "Epoch 18 completed with average loss: 0.3452\n",
            "\n",
            "epoch19/100\n",
            "  Batch 0, Loss: 0.4687\n",
            "  Batch 100, Loss: 0.4950\n",
            "  Batch 200, Loss: 0.3440\n",
            "  Batch 300, Loss: 0.2092\n",
            "  Batch 400, Loss: 0.4186\n",
            "  Batch 500, Loss: 0.3276\n",
            "  Batch 600, Loss: 0.1570\n",
            "  Batch 700, Loss: 0.3815\n",
            "  Batch 800, Loss: 0.2473\n",
            "  Batch 900, Loss: 0.2188\n",
            "  Batch 1000, Loss: 0.3878\n",
            "  Batch 1100, Loss: 0.2464\n",
            "  Batch 1200, Loss: 0.3162\n",
            "  Batch 1300, Loss: 0.4505\n",
            "  Batch 1400, Loss: 0.1843\n",
            "  Batch 1500, Loss: 0.2755\n",
            "  Batch 1600, Loss: 0.1457\n",
            "  Batch 1700, Loss: 0.4037\n",
            "  Batch 1800, Loss: 0.4690\n",
            "Epoch 19 completed with average loss: 0.3403\n",
            "\n",
            "epoch20/100\n",
            "  Batch 0, Loss: 0.2663\n",
            "  Batch 100, Loss: 0.3316\n",
            "  Batch 200, Loss: 0.2244\n",
            "  Batch 300, Loss: 0.2520\n",
            "  Batch 400, Loss: 0.3747\n",
            "  Batch 500, Loss: 0.2365\n",
            "  Batch 600, Loss: 0.2985\n",
            "  Batch 700, Loss: 0.2151\n",
            "  Batch 800, Loss: 0.3992\n",
            "  Batch 900, Loss: 0.2270\n",
            "  Batch 1000, Loss: 0.2493\n",
            "  Batch 1100, Loss: 0.4895\n",
            "  Batch 1200, Loss: 0.1664\n",
            "  Batch 1300, Loss: 0.0980\n",
            "  Batch 1400, Loss: 0.5015\n",
            "  Batch 1500, Loss: 0.2368\n",
            "  Batch 1600, Loss: 0.1601\n",
            "  Batch 1700, Loss: 0.4391\n",
            "  Batch 1800, Loss: 0.2082\n",
            "Epoch 20 completed with average loss: 0.3356\n",
            "\n",
            "epoch21/100\n",
            "  Batch 0, Loss: 0.5382\n",
            "  Batch 100, Loss: 0.2529\n",
            "  Batch 200, Loss: 0.2792\n",
            "  Batch 300, Loss: 0.1929\n",
            "  Batch 400, Loss: 0.1427\n",
            "  Batch 500, Loss: 0.3737\n",
            "  Batch 600, Loss: 0.2579\n",
            "  Batch 700, Loss: 0.4713\n",
            "  Batch 800, Loss: 0.4016\n",
            "  Batch 900, Loss: 0.7024\n",
            "  Batch 1000, Loss: 0.2799\n",
            "  Batch 1100, Loss: 0.2572\n",
            "  Batch 1200, Loss: 0.2288\n",
            "  Batch 1300, Loss: 0.2733\n",
            "  Batch 1400, Loss: 0.2733\n",
            "  Batch 1500, Loss: 0.2238\n",
            "  Batch 1600, Loss: 0.6764\n",
            "  Batch 1700, Loss: 0.1895\n",
            "  Batch 1800, Loss: 0.2957\n",
            "Epoch 21 completed with average loss: 0.3324\n",
            "\n",
            "epoch22/100\n",
            "  Batch 0, Loss: 0.3165\n",
            "  Batch 100, Loss: 0.3733\n",
            "  Batch 200, Loss: 0.4426\n",
            "  Batch 300, Loss: 0.3237\n",
            "  Batch 400, Loss: 0.3140\n",
            "  Batch 500, Loss: 0.2437\n",
            "  Batch 600, Loss: 0.3653\n",
            "  Batch 700, Loss: 0.1927\n",
            "  Batch 800, Loss: 0.2631\n",
            "  Batch 900, Loss: 0.5766\n",
            "  Batch 1000, Loss: 0.1897\n",
            "  Batch 1100, Loss: 0.3159\n",
            "  Batch 1200, Loss: 0.2358\n",
            "  Batch 1300, Loss: 0.2882\n",
            "  Batch 1400, Loss: 0.1545\n",
            "  Batch 1500, Loss: 0.3759\n",
            "  Batch 1600, Loss: 0.4843\n",
            "  Batch 1700, Loss: 0.3575\n",
            "  Batch 1800, Loss: 0.8425\n",
            "Epoch 22 completed with average loss: 0.3276\n",
            "\n",
            "epoch23/100\n",
            "  Batch 0, Loss: 0.3288\n",
            "  Batch 100, Loss: 0.1705\n",
            "  Batch 200, Loss: 0.4738\n",
            "  Batch 300, Loss: 0.3929\n",
            "  Batch 400, Loss: 0.5930\n",
            "  Batch 500, Loss: 0.3445\n",
            "  Batch 600, Loss: 0.3190\n",
            "  Batch 700, Loss: 0.2103\n",
            "  Batch 800, Loss: 0.2538\n",
            "  Batch 900, Loss: 0.2190\n",
            "  Batch 1000, Loss: 0.1408\n",
            "  Batch 1100, Loss: 0.1711\n",
            "  Batch 1200, Loss: 0.3852\n",
            "  Batch 1300, Loss: 0.2036\n",
            "  Batch 1400, Loss: 0.3128\n",
            "  Batch 1500, Loss: 0.5995\n",
            "  Batch 1600, Loss: 0.1028\n",
            "  Batch 1700, Loss: 0.4683\n",
            "  Batch 1800, Loss: 0.2622\n",
            "Epoch 23 completed with average loss: 0.3244\n",
            "\n",
            "epoch24/100\n",
            "  Batch 0, Loss: 0.2960\n",
            "  Batch 100, Loss: 0.2498\n",
            "  Batch 200, Loss: 0.2253\n",
            "  Batch 300, Loss: 0.2592\n",
            "  Batch 400, Loss: 0.1420\n",
            "  Batch 500, Loss: 0.1546\n",
            "  Batch 600, Loss: 0.2846\n",
            "  Batch 700, Loss: 0.3383\n",
            "  Batch 800, Loss: 0.2468\n",
            "  Batch 900, Loss: 0.2708\n",
            "  Batch 1000, Loss: 0.2763\n",
            "  Batch 1100, Loss: 0.3581\n",
            "  Batch 1200, Loss: 0.2342\n",
            "  Batch 1300, Loss: 0.3812\n",
            "  Batch 1400, Loss: 0.5811\n",
            "  Batch 1500, Loss: 0.4178\n",
            "  Batch 1600, Loss: 0.3965\n",
            "  Batch 1700, Loss: 0.3269\n",
            "  Batch 1800, Loss: 0.2170\n",
            "Epoch 24 completed with average loss: 0.3193\n",
            "\n",
            "epoch25/100\n",
            "  Batch 0, Loss: 0.0872\n",
            "  Batch 100, Loss: 0.3083\n",
            "  Batch 200, Loss: 0.4089\n",
            "  Batch 300, Loss: 0.1451\n",
            "  Batch 400, Loss: 0.3427\n",
            "  Batch 500, Loss: 0.3427\n",
            "  Batch 600, Loss: 0.3481\n",
            "  Batch 700, Loss: 0.2292\n",
            "  Batch 800, Loss: 0.4721\n",
            "  Batch 900, Loss: 0.3719\n",
            "  Batch 1000, Loss: 0.3358\n",
            "  Batch 1100, Loss: 0.3155\n",
            "  Batch 1200, Loss: 0.4091\n",
            "  Batch 1300, Loss: 0.2452\n",
            "  Batch 1400, Loss: 0.1708\n",
            "  Batch 1500, Loss: 0.4887\n",
            "  Batch 1600, Loss: 0.5454\n",
            "  Batch 1700, Loss: 0.3559\n",
            "  Batch 1800, Loss: 0.2963\n",
            "Epoch 25 completed with average loss: 0.3162\n",
            "\n",
            "epoch26/100\n",
            "  Batch 0, Loss: 0.1728\n",
            "  Batch 100, Loss: 0.1881\n",
            "  Batch 200, Loss: 0.6542\n",
            "  Batch 300, Loss: 0.3247\n",
            "  Batch 400, Loss: 0.2811\n",
            "  Batch 500, Loss: 0.4797\n",
            "  Batch 600, Loss: 0.2846\n",
            "  Batch 700, Loss: 0.1142\n",
            "  Batch 800, Loss: 0.2636\n",
            "  Batch 900, Loss: 0.2033\n",
            "  Batch 1000, Loss: 0.2266\n",
            "  Batch 1100, Loss: 0.3234\n",
            "  Batch 1200, Loss: 0.3321\n",
            "  Batch 1300, Loss: 0.3853\n",
            "  Batch 1400, Loss: 0.3228\n",
            "  Batch 1500, Loss: 0.3645\n",
            "  Batch 1600, Loss: 0.3195\n",
            "  Batch 1700, Loss: 0.2180\n",
            "  Batch 1800, Loss: 0.2433\n",
            "Epoch 26 completed with average loss: 0.3132\n",
            "\n",
            "epoch27/100\n",
            "  Batch 0, Loss: 0.3928\n",
            "  Batch 100, Loss: 0.4096\n",
            "  Batch 200, Loss: 0.1531\n",
            "  Batch 300, Loss: 0.2819\n",
            "  Batch 400, Loss: 0.3360\n",
            "  Batch 500, Loss: 0.2839\n",
            "  Batch 600, Loss: 0.4345\n",
            "  Batch 700, Loss: 0.2549\n",
            "  Batch 800, Loss: 0.2671\n",
            "  Batch 900, Loss: 0.3579\n",
            "  Batch 1000, Loss: 0.4431\n",
            "  Batch 1100, Loss: 0.2557\n",
            "  Batch 1200, Loss: 0.5056\n",
            "  Batch 1300, Loss: 0.1908\n",
            "  Batch 1400, Loss: 0.3552\n",
            "  Batch 1500, Loss: 0.1802\n",
            "  Batch 1600, Loss: 0.5645\n",
            "  Batch 1700, Loss: 0.1921\n",
            "  Batch 1800, Loss: 0.4268\n",
            "Epoch 27 completed with average loss: 0.3096\n",
            "\n",
            "epoch28/100\n",
            "  Batch 0, Loss: 0.6359\n",
            "  Batch 100, Loss: 0.3408\n",
            "  Batch 200, Loss: 0.3640\n",
            "  Batch 300, Loss: 0.3909\n",
            "  Batch 400, Loss: 0.2606\n",
            "  Batch 500, Loss: 0.3460\n",
            "  Batch 600, Loss: 0.4782\n",
            "  Batch 700, Loss: 0.3244\n",
            "  Batch 800, Loss: 0.5311\n",
            "  Batch 900, Loss: 0.1339\n",
            "  Batch 1000, Loss: 0.1778\n",
            "  Batch 1100, Loss: 0.3919\n",
            "  Batch 1200, Loss: 0.4363\n",
            "  Batch 1300, Loss: 0.1127\n",
            "  Batch 1400, Loss: 0.3248\n",
            "  Batch 1500, Loss: 0.4478\n",
            "  Batch 1600, Loss: 0.4418\n",
            "  Batch 1700, Loss: 0.3064\n",
            "  Batch 1800, Loss: 0.2276\n",
            "Epoch 28 completed with average loss: 0.3067\n",
            "\n",
            "epoch29/100\n",
            "  Batch 0, Loss: 0.1759\n",
            "  Batch 100, Loss: 0.2046\n",
            "  Batch 200, Loss: 0.2121\n",
            "  Batch 300, Loss: 0.3738\n",
            "  Batch 400, Loss: 0.3141\n",
            "  Batch 500, Loss: 0.3422\n",
            "  Batch 600, Loss: 0.1387\n",
            "  Batch 700, Loss: 0.2145\n",
            "  Batch 800, Loss: 0.2682\n",
            "  Batch 900, Loss: 0.7703\n",
            "  Batch 1000, Loss: 0.2035\n",
            "  Batch 1100, Loss: 0.2393\n",
            "  Batch 1200, Loss: 0.3613\n",
            "  Batch 1300, Loss: 0.5490\n",
            "  Batch 1400, Loss: 0.2933\n",
            "  Batch 1500, Loss: 0.5073\n",
            "  Batch 1600, Loss: 0.2293\n",
            "  Batch 1700, Loss: 0.2185\n",
            "  Batch 1800, Loss: 0.1890\n",
            "Epoch 29 completed with average loss: 0.3032\n",
            "\n",
            "epoch30/100\n",
            "  Batch 0, Loss: 0.1922\n",
            "  Batch 100, Loss: 0.3594\n",
            "  Batch 200, Loss: 0.3499\n",
            "  Batch 300, Loss: 0.2665\n",
            "  Batch 400, Loss: 0.4145\n",
            "  Batch 500, Loss: 0.3005\n",
            "  Batch 600, Loss: 0.1389\n",
            "  Batch 700, Loss: 0.2288\n",
            "  Batch 800, Loss: 0.3096\n",
            "  Batch 900, Loss: 0.2634\n",
            "  Batch 1000, Loss: 0.4938\n",
            "  Batch 1100, Loss: 0.1730\n",
            "  Batch 1200, Loss: 0.2241\n",
            "  Batch 1300, Loss: 0.2331\n",
            "  Batch 1400, Loss: 0.2025\n",
            "  Batch 1500, Loss: 0.2342\n",
            "  Batch 1600, Loss: 0.2612\n",
            "  Batch 1700, Loss: 0.2310\n",
            "  Batch 1800, Loss: 0.2275\n",
            "Epoch 30 completed with average loss: 0.3003\n",
            "\n",
            "epoch31/100\n",
            "  Batch 0, Loss: 0.2952\n",
            "  Batch 100, Loss: 0.2875\n",
            "  Batch 200, Loss: 0.1808\n",
            "  Batch 300, Loss: 0.1134\n",
            "  Batch 400, Loss: 0.4634\n",
            "  Batch 500, Loss: 0.3353\n",
            "  Batch 600, Loss: 0.3790\n",
            "  Batch 700, Loss: 0.1927\n",
            "  Batch 800, Loss: 0.1354\n",
            "  Batch 900, Loss: 0.2321\n",
            "  Batch 1000, Loss: 0.3241\n",
            "  Batch 1100, Loss: 0.1509\n",
            "  Batch 1200, Loss: 0.3081\n",
            "  Batch 1300, Loss: 0.2447\n",
            "  Batch 1400, Loss: 0.1726\n",
            "  Batch 1500, Loss: 0.3181\n",
            "  Batch 1600, Loss: 0.1650\n",
            "  Batch 1700, Loss: 0.2810\n",
            "  Batch 1800, Loss: 0.3401\n",
            "Epoch 31 completed with average loss: 0.2978\n",
            "\n",
            "epoch32/100\n",
            "  Batch 0, Loss: 0.2238\n",
            "  Batch 100, Loss: 0.2494\n",
            "  Batch 200, Loss: 0.1911\n",
            "  Batch 300, Loss: 0.2545\n",
            "  Batch 400, Loss: 0.4052\n",
            "  Batch 500, Loss: 0.1466\n",
            "  Batch 600, Loss: 0.1910\n",
            "  Batch 700, Loss: 0.1650\n",
            "  Batch 800, Loss: 0.1174\n",
            "  Batch 900, Loss: 0.1126\n",
            "  Batch 1000, Loss: 0.2033\n",
            "  Batch 1100, Loss: 0.2558\n",
            "  Batch 1200, Loss: 0.2502\n",
            "  Batch 1300, Loss: 0.9319\n",
            "  Batch 1400, Loss: 0.3136\n",
            "  Batch 1500, Loss: 0.4233\n",
            "  Batch 1600, Loss: 0.1944\n",
            "  Batch 1700, Loss: 0.2617\n",
            "  Batch 1800, Loss: 0.1652\n",
            "Epoch 32 completed with average loss: 0.2947\n",
            "\n",
            "epoch33/100\n",
            "  Batch 0, Loss: 0.1855\n",
            "  Batch 100, Loss: 0.2651\n",
            "  Batch 200, Loss: 0.2827\n",
            "  Batch 300, Loss: 0.2180\n",
            "  Batch 400, Loss: 0.2118\n",
            "  Batch 500, Loss: 0.2045\n",
            "  Batch 600, Loss: 0.3066\n",
            "  Batch 700, Loss: 0.3310\n",
            "  Batch 800, Loss: 0.2170\n",
            "  Batch 900, Loss: 0.1773\n",
            "  Batch 1000, Loss: 0.1600\n",
            "  Batch 1100, Loss: 0.2817\n",
            "  Batch 1200, Loss: 0.3050\n",
            "  Batch 1300, Loss: 0.2951\n",
            "  Batch 1400, Loss: 0.4835\n",
            "  Batch 1500, Loss: 0.4905\n",
            "  Batch 1600, Loss: 0.3677\n",
            "  Batch 1700, Loss: 0.2658\n",
            "  Batch 1800, Loss: 0.3229\n",
            "Epoch 33 completed with average loss: 0.2915\n",
            "\n",
            "epoch34/100\n",
            "  Batch 0, Loss: 0.2470\n",
            "  Batch 100, Loss: 0.3124\n",
            "  Batch 200, Loss: 0.2812\n",
            "  Batch 300, Loss: 0.4844\n",
            "  Batch 400, Loss: 0.2048\n",
            "  Batch 500, Loss: 0.3105\n",
            "  Batch 600, Loss: 0.2624\n",
            "  Batch 700, Loss: 0.2111\n",
            "  Batch 800, Loss: 0.2434\n",
            "  Batch 900, Loss: 0.1868\n",
            "  Batch 1000, Loss: 0.2243\n",
            "  Batch 1100, Loss: 0.4079\n",
            "  Batch 1200, Loss: 0.3645\n",
            "  Batch 1300, Loss: 0.4405\n",
            "  Batch 1400, Loss: 0.1832\n",
            "  Batch 1500, Loss: 0.2393\n",
            "  Batch 1600, Loss: 0.2220\n",
            "  Batch 1700, Loss: 0.3596\n",
            "  Batch 1800, Loss: 0.1595\n",
            "Epoch 34 completed with average loss: 0.2892\n",
            "\n",
            "epoch35/100\n",
            "  Batch 0, Loss: 0.2563\n",
            "  Batch 100, Loss: 0.3671\n",
            "  Batch 200, Loss: 0.1353\n",
            "  Batch 300, Loss: 0.3417\n",
            "  Batch 400, Loss: 0.3135\n",
            "  Batch 500, Loss: 0.1868\n",
            "  Batch 600, Loss: 0.2148\n",
            "  Batch 700, Loss: 0.3990\n",
            "  Batch 800, Loss: 0.1193\n",
            "  Batch 900, Loss: 0.3124\n",
            "  Batch 1000, Loss: 0.2789\n",
            "  Batch 1100, Loss: 0.4019\n",
            "  Batch 1200, Loss: 0.2999\n",
            "  Batch 1300, Loss: 0.3984\n",
            "  Batch 1400, Loss: 0.3544\n",
            "  Batch 1500, Loss: 0.2123\n",
            "  Batch 1600, Loss: 0.2863\n",
            "  Batch 1700, Loss: 0.3150\n",
            "  Batch 1800, Loss: 0.3406\n",
            "Epoch 35 completed with average loss: 0.2862\n",
            "\n",
            "epoch36/100\n",
            "  Batch 0, Loss: 0.1742\n",
            "  Batch 100, Loss: 0.3163\n",
            "  Batch 200, Loss: 0.2534\n",
            "  Batch 300, Loss: 0.3571\n",
            "  Batch 400, Loss: 0.1366\n",
            "  Batch 500, Loss: 0.1584\n",
            "  Batch 600, Loss: 0.1855\n",
            "  Batch 700, Loss: 0.4634\n",
            "  Batch 800, Loss: 0.4125\n",
            "  Batch 900, Loss: 0.3958\n",
            "  Batch 1000, Loss: 0.1840\n",
            "  Batch 1100, Loss: 0.2812\n",
            "  Batch 1200, Loss: 0.4630\n",
            "  Batch 1300, Loss: 0.2204\n",
            "  Batch 1400, Loss: 0.1997\n",
            "  Batch 1500, Loss: 0.2601\n",
            "  Batch 1600, Loss: 0.2442\n",
            "  Batch 1700, Loss: 0.2300\n",
            "  Batch 1800, Loss: 0.2860\n",
            "Epoch 36 completed with average loss: 0.2839\n",
            "\n",
            "epoch37/100\n",
            "  Batch 0, Loss: 0.2382\n",
            "  Batch 100, Loss: 0.1343\n",
            "  Batch 200, Loss: 0.3645\n",
            "  Batch 300, Loss: 0.2840\n",
            "  Batch 400, Loss: 0.3566\n",
            "  Batch 500, Loss: 0.1157\n",
            "  Batch 600, Loss: 0.2594\n",
            "  Batch 700, Loss: 0.4507\n",
            "  Batch 800, Loss: 0.0915\n",
            "  Batch 900, Loss: 0.3626\n",
            "  Batch 1000, Loss: 0.3677\n",
            "  Batch 1100, Loss: 0.2910\n",
            "  Batch 1200, Loss: 0.4241\n",
            "  Batch 1300, Loss: 0.2961\n",
            "  Batch 1400, Loss: 0.3634\n",
            "  Batch 1500, Loss: 0.3832\n",
            "  Batch 1600, Loss: 0.4243\n",
            "  Batch 1700, Loss: 0.2851\n",
            "  Batch 1800, Loss: 0.1445\n",
            "Epoch 37 completed with average loss: 0.2812\n",
            "\n",
            "epoch38/100\n",
            "  Batch 0, Loss: 0.2792\n",
            "  Batch 100, Loss: 0.4527\n",
            "  Batch 200, Loss: 0.3426\n",
            "  Batch 300, Loss: 0.2335\n",
            "  Batch 400, Loss: 0.2726\n",
            "  Batch 500, Loss: 0.1787\n",
            "  Batch 600, Loss: 0.3314\n",
            "  Batch 700, Loss: 0.2399\n",
            "  Batch 800, Loss: 0.2378\n",
            "  Batch 900, Loss: 0.2110\n",
            "  Batch 1000, Loss: 0.3230\n",
            "  Batch 1100, Loss: 0.2309\n",
            "  Batch 1200, Loss: 0.5262\n",
            "  Batch 1300, Loss: 0.3256\n",
            "  Batch 1400, Loss: 0.3220\n",
            "  Batch 1500, Loss: 0.2031\n",
            "  Batch 1600, Loss: 0.1997\n",
            "  Batch 1700, Loss: 0.0825\n",
            "  Batch 1800, Loss: 0.3661\n",
            "Epoch 38 completed with average loss: 0.2780\n",
            "\n",
            "epoch39/100\n",
            "  Batch 0, Loss: 0.1674\n",
            "  Batch 100, Loss: 0.1901\n",
            "  Batch 200, Loss: 0.2681\n",
            "  Batch 300, Loss: 0.2009\n",
            "  Batch 400, Loss: 0.3536\n",
            "  Batch 500, Loss: 0.2070\n",
            "  Batch 600, Loss: 0.1509\n",
            "  Batch 700, Loss: 0.3037\n",
            "  Batch 800, Loss: 0.1548\n",
            "  Batch 900, Loss: 0.2075\n",
            "  Batch 1000, Loss: 0.2316\n",
            "  Batch 1100, Loss: 0.2622\n",
            "  Batch 1200, Loss: 0.3470\n",
            "  Batch 1300, Loss: 0.2265\n",
            "  Batch 1400, Loss: 0.2536\n",
            "  Batch 1500, Loss: 0.2525\n",
            "  Batch 1600, Loss: 0.2289\n",
            "  Batch 1700, Loss: 0.2991\n",
            "  Batch 1800, Loss: 0.4193\n",
            "Epoch 39 completed with average loss: 0.2762\n",
            "\n",
            "epoch40/100\n",
            "  Batch 0, Loss: 0.3396\n",
            "  Batch 100, Loss: 0.1599\n",
            "  Batch 200, Loss: 0.1137\n",
            "  Batch 300, Loss: 0.4619\n",
            "  Batch 400, Loss: 0.1653\n",
            "  Batch 500, Loss: 0.4375\n",
            "  Batch 600, Loss: 0.4232\n",
            "  Batch 700, Loss: 0.1848\n",
            "  Batch 800, Loss: 0.2359\n",
            "  Batch 900, Loss: 0.3148\n",
            "  Batch 1000, Loss: 0.2037\n",
            "  Batch 1100, Loss: 0.1479\n",
            "  Batch 1200, Loss: 0.3403\n",
            "  Batch 1300, Loss: 0.1451\n",
            "  Batch 1400, Loss: 0.1947\n",
            "  Batch 1500, Loss: 0.3463\n",
            "  Batch 1600, Loss: 0.1797\n",
            "  Batch 1700, Loss: 0.3032\n",
            "  Batch 1800, Loss: 0.1714\n",
            "Epoch 40 completed with average loss: 0.2740\n",
            "\n",
            "epoch41/100\n",
            "  Batch 0, Loss: 0.3184\n",
            "  Batch 100, Loss: 0.2791\n",
            "  Batch 200, Loss: 0.2071\n",
            "  Batch 300, Loss: 0.2299\n",
            "  Batch 400, Loss: 0.1445\n",
            "  Batch 500, Loss: 0.1787\n",
            "  Batch 600, Loss: 0.6718\n",
            "  Batch 700, Loss: 0.4975\n",
            "  Batch 800, Loss: 0.1597\n",
            "  Batch 900, Loss: 0.1133\n",
            "  Batch 1000, Loss: 0.4443\n",
            "  Batch 1100, Loss: 0.2841\n",
            "  Batch 1200, Loss: 0.2039\n",
            "  Batch 1300, Loss: 0.1544\n",
            "  Batch 1400, Loss: 0.2458\n",
            "  Batch 1500, Loss: 0.2747\n",
            "  Batch 1600, Loss: 0.3918\n",
            "  Batch 1700, Loss: 0.2325\n",
            "  Batch 1800, Loss: 0.2709\n",
            "Epoch 41 completed with average loss: 0.2716\n",
            "\n",
            "epoch42/100\n",
            "  Batch 0, Loss: 0.4073\n",
            "  Batch 100, Loss: 0.2935\n",
            "  Batch 200, Loss: 0.3347\n",
            "  Batch 300, Loss: 0.5008\n",
            "  Batch 400, Loss: 0.3674\n",
            "  Batch 500, Loss: 0.2231\n",
            "  Batch 600, Loss: 0.2489\n",
            "  Batch 700, Loss: 0.1369\n",
            "  Batch 800, Loss: 0.3328\n",
            "  Batch 900, Loss: 0.3247\n",
            "  Batch 1000, Loss: 0.3055\n",
            "  Batch 1100, Loss: 0.1997\n",
            "  Batch 1200, Loss: 0.3442\n",
            "  Batch 1300, Loss: 0.2339\n",
            "  Batch 1400, Loss: 0.3637\n",
            "  Batch 1500, Loss: 0.1911\n",
            "  Batch 1600, Loss: 0.1510\n",
            "  Batch 1700, Loss: 0.2069\n",
            "  Batch 1800, Loss: 0.2030\n",
            "Epoch 42 completed with average loss: 0.2686\n",
            "\n",
            "epoch43/100\n",
            "  Batch 0, Loss: 0.2458\n",
            "  Batch 100, Loss: 0.2992\n",
            "  Batch 200, Loss: 0.2308\n",
            "  Batch 300, Loss: 0.3013\n",
            "  Batch 400, Loss: 0.1522\n",
            "  Batch 500, Loss: 0.3022\n",
            "  Batch 600, Loss: 0.2637\n",
            "  Batch 700, Loss: 0.2337\n",
            "  Batch 800, Loss: 0.2297\n",
            "  Batch 900, Loss: 0.2178\n",
            "  Batch 1000, Loss: 0.0904\n",
            "  Batch 1100, Loss: 0.2285\n",
            "  Batch 1200, Loss: 0.2806\n",
            "  Batch 1300, Loss: 0.2837\n",
            "  Batch 1400, Loss: 0.1474\n",
            "  Batch 1500, Loss: 0.2126\n",
            "  Batch 1600, Loss: 0.3619\n",
            "  Batch 1700, Loss: 0.2790\n",
            "  Batch 1800, Loss: 0.2175\n",
            "Epoch 43 completed with average loss: 0.2664\n",
            "\n",
            "epoch44/100\n",
            "  Batch 0, Loss: 0.0749\n",
            "  Batch 100, Loss: 0.1377\n",
            "  Batch 200, Loss: 0.2579\n",
            "  Batch 300, Loss: 0.4085\n",
            "  Batch 400, Loss: 0.5128\n",
            "  Batch 500, Loss: 0.4620\n",
            "  Batch 600, Loss: 0.1934\n",
            "  Batch 700, Loss: 0.0744\n",
            "  Batch 800, Loss: 0.1185\n",
            "  Batch 900, Loss: 0.1990\n",
            "  Batch 1000, Loss: 0.1300\n",
            "  Batch 1100, Loss: 0.1868\n",
            "  Batch 1200, Loss: 0.1771\n",
            "  Batch 1300, Loss: 0.2461\n",
            "  Batch 1400, Loss: 0.1512\n",
            "  Batch 1500, Loss: 0.4976\n",
            "  Batch 1600, Loss: 0.2720\n",
            "  Batch 1700, Loss: 0.2977\n",
            "  Batch 1800, Loss: 0.1470\n",
            "Epoch 44 completed with average loss: 0.2641\n",
            "\n",
            "epoch45/100\n",
            "  Batch 0, Loss: 0.1485\n",
            "  Batch 100, Loss: 0.1887\n",
            "  Batch 200, Loss: 0.3402\n",
            "  Batch 300, Loss: 0.5292\n",
            "  Batch 400, Loss: 0.3885\n",
            "  Batch 500, Loss: 0.2620\n",
            "  Batch 600, Loss: 0.2550\n",
            "  Batch 700, Loss: 0.2712\n",
            "  Batch 800, Loss: 0.4067\n",
            "  Batch 900, Loss: 0.2986\n",
            "  Batch 1000, Loss: 0.1768\n",
            "  Batch 1100, Loss: 0.2781\n",
            "  Batch 1200, Loss: 0.1123\n",
            "  Batch 1300, Loss: 0.2511\n",
            "  Batch 1400, Loss: 0.2605\n",
            "  Batch 1500, Loss: 0.0862\n",
            "  Batch 1600, Loss: 0.5218\n",
            "  Batch 1700, Loss: 0.3191\n",
            "  Batch 1800, Loss: 0.2980\n",
            "Epoch 45 completed with average loss: 0.2623\n",
            "\n",
            "epoch46/100\n",
            "  Batch 0, Loss: 0.3285\n",
            "  Batch 100, Loss: 0.2902\n",
            "  Batch 200, Loss: 0.1038\n",
            "  Batch 300, Loss: 0.4228\n",
            "  Batch 400, Loss: 0.3071\n",
            "  Batch 500, Loss: 0.2424\n",
            "  Batch 600, Loss: 0.3868\n",
            "  Batch 700, Loss: 0.1903\n",
            "  Batch 800, Loss: 0.3885\n",
            "  Batch 900, Loss: 0.2580\n",
            "  Batch 1000, Loss: 0.3164\n",
            "  Batch 1100, Loss: 0.1365\n",
            "  Batch 1200, Loss: 0.6056\n",
            "  Batch 1300, Loss: 0.1911\n",
            "  Batch 1400, Loss: 0.2455\n",
            "  Batch 1500, Loss: 0.1179\n",
            "  Batch 1600, Loss: 0.3349\n",
            "  Batch 1700, Loss: 0.3926\n",
            "  Batch 1800, Loss: 0.0572\n",
            "Epoch 46 completed with average loss: 0.2602\n",
            "\n",
            "epoch47/100\n",
            "  Batch 0, Loss: 0.2425\n",
            "  Batch 100, Loss: 0.2529\n",
            "  Batch 200, Loss: 0.3028\n",
            "  Batch 300, Loss: 0.4210\n",
            "  Batch 400, Loss: 0.2066\n",
            "  Batch 500, Loss: 0.1544\n",
            "  Batch 600, Loss: 0.3288\n",
            "  Batch 700, Loss: 0.1316\n",
            "  Batch 800, Loss: 0.3249\n",
            "  Batch 900, Loss: 0.3092\n",
            "  Batch 1000, Loss: 0.5868\n",
            "  Batch 1100, Loss: 0.3844\n",
            "  Batch 1200, Loss: 0.2657\n",
            "  Batch 1300, Loss: 0.1761\n",
            "  Batch 1400, Loss: 0.2677\n",
            "  Batch 1500, Loss: 0.3433\n",
            "  Batch 1600, Loss: 0.2479\n",
            "  Batch 1700, Loss: 0.4477\n",
            "  Batch 1800, Loss: 0.3354\n",
            "Epoch 47 completed with average loss: 0.2584\n",
            "\n",
            "epoch48/100\n",
            "  Batch 0, Loss: 0.2602\n",
            "  Batch 100, Loss: 0.1803\n",
            "  Batch 200, Loss: 0.1605\n",
            "  Batch 300, Loss: 0.3530\n",
            "  Batch 400, Loss: 0.1073\n",
            "  Batch 500, Loss: 0.3307\n",
            "  Batch 600, Loss: 0.2031\n",
            "  Batch 700, Loss: 0.3263\n",
            "  Batch 800, Loss: 0.3229\n",
            "  Batch 900, Loss: 0.2163\n",
            "  Batch 1000, Loss: 0.1252\n",
            "  Batch 1100, Loss: 0.2577\n",
            "  Batch 1200, Loss: 0.2870\n",
            "  Batch 1300, Loss: 0.1550\n",
            "  Batch 1400, Loss: 0.3823\n",
            "  Batch 1500, Loss: 0.3244\n",
            "  Batch 1600, Loss: 0.1742\n",
            "  Batch 1700, Loss: 0.3071\n",
            "  Batch 1800, Loss: 0.2885\n",
            "Epoch 48 completed with average loss: 0.2564\n",
            "\n",
            "epoch49/100\n",
            "  Batch 0, Loss: 0.0614\n",
            "  Batch 100, Loss: 0.2598\n",
            "  Batch 200, Loss: 0.1862\n",
            "  Batch 300, Loss: 0.4010\n",
            "  Batch 400, Loss: 0.1224\n",
            "  Batch 500, Loss: 0.2090\n",
            "  Batch 600, Loss: 0.0860\n",
            "  Batch 700, Loss: 0.1296\n",
            "  Batch 800, Loss: 0.2868\n",
            "  Batch 900, Loss: 0.0885\n",
            "  Batch 1000, Loss: 0.3008\n",
            "  Batch 1100, Loss: 0.2213\n",
            "  Batch 1200, Loss: 0.2177\n",
            "  Batch 1300, Loss: 0.2135\n",
            "  Batch 1400, Loss: 0.1933\n",
            "  Batch 1500, Loss: 0.1400\n",
            "  Batch 1600, Loss: 0.2654\n",
            "  Batch 1700, Loss: 0.2174\n",
            "  Batch 1800, Loss: 0.1746\n",
            "Epoch 49 completed with average loss: 0.2543\n",
            "\n",
            "epoch50/100\n",
            "  Batch 0, Loss: 0.2836\n",
            "  Batch 100, Loss: 0.1980\n",
            "  Batch 200, Loss: 0.1457\n",
            "  Batch 300, Loss: 0.2207\n",
            "  Batch 400, Loss: 0.1110\n",
            "  Batch 500, Loss: 0.1330\n",
            "  Batch 600, Loss: 0.3643\n",
            "  Batch 700, Loss: 0.0890\n",
            "  Batch 800, Loss: 0.1831\n",
            "  Batch 900, Loss: 0.2818\n",
            "  Batch 1000, Loss: 0.1732\n",
            "  Batch 1100, Loss: 0.2321\n",
            "  Batch 1200, Loss: 0.1380\n",
            "  Batch 1300, Loss: 0.2446\n",
            "  Batch 1400, Loss: 0.2466\n",
            "  Batch 1500, Loss: 0.2159\n",
            "  Batch 1600, Loss: 0.3736\n",
            "  Batch 1700, Loss: 0.3583\n",
            "  Batch 1800, Loss: 0.2121\n",
            "Epoch 50 completed with average loss: 0.2522\n",
            "\n",
            "epoch51/100\n",
            "  Batch 0, Loss: 0.2836\n",
            "  Batch 100, Loss: 0.2293\n",
            "  Batch 200, Loss: 0.2577\n",
            "  Batch 300, Loss: 0.2068\n",
            "  Batch 400, Loss: 0.2560\n",
            "  Batch 500, Loss: 0.2510\n",
            "  Batch 600, Loss: 0.1369\n",
            "  Batch 700, Loss: 0.3875\n",
            "  Batch 800, Loss: 0.1094\n",
            "  Batch 900, Loss: 0.1174\n",
            "  Batch 1000, Loss: 0.2966\n",
            "  Batch 1100, Loss: 0.3749\n",
            "  Batch 1200, Loss: 0.2728\n",
            "  Batch 1300, Loss: 0.2152\n",
            "  Batch 1400, Loss: 0.5200\n",
            "  Batch 1500, Loss: 0.1992\n",
            "  Batch 1600, Loss: 0.2538\n",
            "  Batch 1700, Loss: 0.2266\n",
            "  Batch 1800, Loss: 0.4134\n",
            "Epoch 51 completed with average loss: 0.2501\n",
            "\n",
            "epoch52/100\n",
            "  Batch 0, Loss: 0.4783\n",
            "  Batch 100, Loss: 0.4074\n",
            "  Batch 200, Loss: 0.2793\n",
            "  Batch 300, Loss: 0.3667\n",
            "  Batch 400, Loss: 0.1967\n",
            "  Batch 500, Loss: 0.1296\n",
            "  Batch 600, Loss: 0.2453\n",
            "  Batch 700, Loss: 0.2742\n",
            "  Batch 800, Loss: 0.2280\n",
            "  Batch 900, Loss: 0.1678\n",
            "  Batch 1000, Loss: 0.1143\n",
            "  Batch 1100, Loss: 0.2608\n",
            "  Batch 1200, Loss: 0.2519\n",
            "  Batch 1300, Loss: 0.1927\n",
            "  Batch 1400, Loss: 0.2880\n",
            "  Batch 1500, Loss: 0.1503\n",
            "  Batch 1600, Loss: 0.3049\n",
            "  Batch 1700, Loss: 0.2367\n",
            "  Batch 1800, Loss: 0.3397\n",
            "Epoch 52 completed with average loss: 0.2485\n",
            "\n",
            "epoch53/100\n",
            "  Batch 0, Loss: 0.4450\n",
            "  Batch 100, Loss: 0.1650\n",
            "  Batch 200, Loss: 0.2803\n",
            "  Batch 300, Loss: 0.2392\n",
            "  Batch 400, Loss: 0.3048\n",
            "  Batch 500, Loss: 0.2546\n",
            "  Batch 600, Loss: 0.0938\n",
            "  Batch 700, Loss: 0.1864\n",
            "  Batch 800, Loss: 0.1658\n",
            "  Batch 900, Loss: 0.3249\n",
            "  Batch 1000, Loss: 0.0879\n",
            "  Batch 1100, Loss: 0.5655\n",
            "  Batch 1200, Loss: 0.1990\n",
            "  Batch 1300, Loss: 0.4542\n",
            "  Batch 1400, Loss: 0.1291\n",
            "  Batch 1500, Loss: 0.2032\n",
            "  Batch 1600, Loss: 0.1372\n",
            "  Batch 1700, Loss: 0.1998\n",
            "  Batch 1800, Loss: 0.2203\n",
            "Epoch 53 completed with average loss: 0.2463\n",
            "\n",
            "epoch54/100\n",
            "  Batch 0, Loss: 0.3564\n",
            "  Batch 100, Loss: 0.2576\n",
            "  Batch 200, Loss: 0.1194\n",
            "  Batch 300, Loss: 0.1202\n",
            "  Batch 400, Loss: 0.3416\n",
            "  Batch 500, Loss: 0.0953\n",
            "  Batch 600, Loss: 0.2702\n",
            "  Batch 700, Loss: 0.4095\n",
            "  Batch 800, Loss: 0.4684\n",
            "  Batch 900, Loss: 0.1961\n",
            "  Batch 1000, Loss: 0.0886\n",
            "  Batch 1100, Loss: 0.2035\n",
            "  Batch 1200, Loss: 0.5029\n",
            "  Batch 1300, Loss: 0.1136\n",
            "  Batch 1400, Loss: 0.0550\n",
            "  Batch 1500, Loss: 0.1979\n",
            "  Batch 1600, Loss: 0.1151\n",
            "  Batch 1700, Loss: 0.2839\n",
            "  Batch 1800, Loss: 0.0988\n",
            "Epoch 54 completed with average loss: 0.2448\n",
            "\n",
            "epoch55/100\n",
            "  Batch 0, Loss: 0.2329\n",
            "  Batch 100, Loss: 0.2231\n",
            "  Batch 200, Loss: 0.2359\n",
            "  Batch 300, Loss: 0.3345\n",
            "  Batch 400, Loss: 0.1566\n",
            "  Batch 500, Loss: 0.4307\n",
            "  Batch 600, Loss: 0.3780\n",
            "  Batch 700, Loss: 0.1070\n",
            "  Batch 800, Loss: 0.1584\n",
            "  Batch 900, Loss: 0.3629\n",
            "  Batch 1000, Loss: 0.2558\n",
            "  Batch 1100, Loss: 0.3704\n",
            "  Batch 1200, Loss: 0.3317\n",
            "  Batch 1300, Loss: 0.1904\n",
            "  Batch 1400, Loss: 0.1010\n",
            "  Batch 1500, Loss: 0.3271\n",
            "  Batch 1600, Loss: 0.1291\n",
            "  Batch 1700, Loss: 0.1764\n",
            "  Batch 1800, Loss: 0.1645\n",
            "Epoch 55 completed with average loss: 0.2433\n",
            "\n",
            "epoch56/100\n",
            "  Batch 0, Loss: 0.2108\n",
            "  Batch 100, Loss: 0.0756\n",
            "  Batch 200, Loss: 0.2337\n",
            "  Batch 300, Loss: 0.0807\n",
            "  Batch 400, Loss: 0.2349\n",
            "  Batch 500, Loss: 0.1882\n",
            "  Batch 600, Loss: 0.3005\n",
            "  Batch 700, Loss: 0.2308\n",
            "  Batch 800, Loss: 0.2079\n",
            "  Batch 900, Loss: 0.2449\n",
            "  Batch 1000, Loss: 0.2568\n",
            "  Batch 1100, Loss: 0.2792\n",
            "  Batch 1200, Loss: 0.0749\n",
            "  Batch 1300, Loss: 0.4258\n",
            "  Batch 1400, Loss: 0.2041\n",
            "  Batch 1500, Loss: 0.1834\n",
            "  Batch 1600, Loss: 0.3398\n",
            "  Batch 1700, Loss: 0.2638\n",
            "  Batch 1800, Loss: 0.1136\n",
            "Epoch 56 completed with average loss: 0.2405\n",
            "\n",
            "epoch57/100\n",
            "  Batch 0, Loss: 0.2966\n",
            "  Batch 100, Loss: 0.1893\n",
            "  Batch 200, Loss: 0.1830\n",
            "  Batch 300, Loss: 0.2647\n",
            "  Batch 400, Loss: 0.3303\n",
            "  Batch 500, Loss: 0.3477\n",
            "  Batch 600, Loss: 0.1599\n",
            "  Batch 700, Loss: 0.0671\n",
            "  Batch 800, Loss: 0.4388\n",
            "  Batch 900, Loss: 0.2817\n",
            "  Batch 1000, Loss: 0.1346\n",
            "  Batch 1100, Loss: 0.1305\n",
            "  Batch 1200, Loss: 0.2562\n",
            "  Batch 1300, Loss: 0.2408\n",
            "  Batch 1400, Loss: 0.1845\n",
            "  Batch 1500, Loss: 0.3255\n",
            "  Batch 1600, Loss: 0.3198\n",
            "  Batch 1700, Loss: 0.2644\n",
            "  Batch 1800, Loss: 0.0742\n",
            "Epoch 57 completed with average loss: 0.2391\n",
            "\n",
            "epoch58/100\n",
            "  Batch 0, Loss: 0.1939\n",
            "  Batch 100, Loss: 0.3698\n",
            "  Batch 200, Loss: 0.1160\n",
            "  Batch 300, Loss: 0.2882\n",
            "  Batch 400, Loss: 0.1882\n",
            "  Batch 500, Loss: 0.0889\n",
            "  Batch 600, Loss: 0.3392\n",
            "  Batch 700, Loss: 0.2223\n",
            "  Batch 800, Loss: 0.1152\n",
            "  Batch 900, Loss: 0.2781\n",
            "  Batch 1000, Loss: 0.2654\n",
            "  Batch 1100, Loss: 0.0757\n",
            "  Batch 1200, Loss: 0.1685\n",
            "  Batch 1300, Loss: 0.3084\n",
            "  Batch 1400, Loss: 0.2913\n",
            "  Batch 1500, Loss: 0.3550\n",
            "  Batch 1600, Loss: 0.1451\n",
            "  Batch 1700, Loss: 0.2123\n",
            "  Batch 1800, Loss: 0.4696\n",
            "Epoch 58 completed with average loss: 0.2377\n",
            "\n",
            "epoch59/100\n",
            "  Batch 0, Loss: 0.3123\n",
            "  Batch 100, Loss: 0.2883\n",
            "  Batch 200, Loss: 0.3004\n",
            "  Batch 300, Loss: 0.4096\n",
            "  Batch 400, Loss: 0.1393\n",
            "  Batch 500, Loss: 0.1015\n",
            "  Batch 600, Loss: 0.3632\n",
            "  Batch 700, Loss: 0.3233\n",
            "  Batch 800, Loss: 0.2558\n",
            "  Batch 900, Loss: 0.2985\n",
            "  Batch 1000, Loss: 0.2211\n",
            "  Batch 1100, Loss: 0.2850\n",
            "  Batch 1200, Loss: 0.1789\n",
            "  Batch 1300, Loss: 0.4072\n",
            "  Batch 1400, Loss: 0.1581\n",
            "  Batch 1500, Loss: 0.1436\n",
            "  Batch 1600, Loss: 0.2966\n",
            "  Batch 1700, Loss: 0.1737\n",
            "  Batch 1800, Loss: 0.2182\n",
            "Epoch 59 completed with average loss: 0.2359\n",
            "\n",
            "epoch60/100\n",
            "  Batch 0, Loss: 0.2760\n",
            "  Batch 100, Loss: 0.2296\n",
            "  Batch 200, Loss: 0.1180\n",
            "  Batch 300, Loss: 0.1658\n",
            "  Batch 400, Loss: 0.4220\n",
            "  Batch 500, Loss: 0.4175\n",
            "  Batch 600, Loss: 0.2877\n",
            "  Batch 700, Loss: 0.2428\n",
            "  Batch 800, Loss: 0.2387\n",
            "  Batch 900, Loss: 0.1299\n",
            "  Batch 1000, Loss: 0.4713\n",
            "  Batch 1100, Loss: 0.1650\n",
            "  Batch 1200, Loss: 0.1833\n",
            "  Batch 1300, Loss: 0.1596\n",
            "  Batch 1400, Loss: 0.3292\n",
            "  Batch 1500, Loss: 0.1985\n",
            "  Batch 1600, Loss: 0.3086\n",
            "  Batch 1700, Loss: 0.5935\n",
            "  Batch 1800, Loss: 0.2923\n",
            "Epoch 60 completed with average loss: 0.2346\n",
            "\n",
            "epoch61/100\n",
            "  Batch 0, Loss: 0.3992\n",
            "  Batch 100, Loss: 0.2386\n",
            "  Batch 200, Loss: 0.2240\n",
            "  Batch 300, Loss: 0.3434\n",
            "  Batch 400, Loss: 0.1951\n",
            "  Batch 500, Loss: 0.1732\n",
            "  Batch 600, Loss: 0.2945\n",
            "  Batch 700, Loss: 0.1667\n",
            "  Batch 800, Loss: 0.2756\n",
            "  Batch 900, Loss: 0.1845\n",
            "  Batch 1000, Loss: 0.3643\n",
            "  Batch 1100, Loss: 0.0664\n",
            "  Batch 1200, Loss: 0.3547\n",
            "  Batch 1300, Loss: 0.3221\n",
            "  Batch 1400, Loss: 0.3179\n",
            "  Batch 1500, Loss: 0.1860\n",
            "  Batch 1600, Loss: 0.3845\n",
            "  Batch 1700, Loss: 0.2941\n",
            "  Batch 1800, Loss: 0.3298\n",
            "Epoch 61 completed with average loss: 0.2324\n",
            "\n",
            "epoch62/100\n",
            "  Batch 0, Loss: 0.2107\n",
            "  Batch 100, Loss: 0.2079\n",
            "  Batch 200, Loss: 0.2294\n",
            "  Batch 300, Loss: 0.2303\n",
            "  Batch 400, Loss: 0.3657\n",
            "  Batch 500, Loss: 0.2485\n",
            "  Batch 600, Loss: 0.2007\n",
            "  Batch 700, Loss: 0.0960\n",
            "  Batch 800, Loss: 0.1434\n",
            "  Batch 900, Loss: 0.1372\n",
            "  Batch 1000, Loss: 0.1556\n",
            "  Batch 1100, Loss: 0.2296\n",
            "  Batch 1200, Loss: 0.1640\n",
            "  Batch 1300, Loss: 0.3668\n",
            "  Batch 1400, Loss: 0.1218\n",
            "  Batch 1500, Loss: 0.0977\n",
            "  Batch 1600, Loss: 0.1253\n",
            "  Batch 1700, Loss: 0.1336\n",
            "  Batch 1800, Loss: 0.1469\n",
            "Epoch 62 completed with average loss: 0.2309\n",
            "\n",
            "epoch63/100\n",
            "  Batch 0, Loss: 0.1149\n",
            "  Batch 100, Loss: 0.0660\n",
            "  Batch 200, Loss: 0.1236\n",
            "  Batch 300, Loss: 0.3954\n",
            "  Batch 400, Loss: 0.2923\n",
            "  Batch 500, Loss: 0.2046\n",
            "  Batch 600, Loss: 0.2100\n",
            "  Batch 700, Loss: 0.2426\n",
            "  Batch 800, Loss: 0.1389\n",
            "  Batch 900, Loss: 0.2446\n",
            "  Batch 1000, Loss: 0.0862\n",
            "  Batch 1100, Loss: 0.2145\n",
            "  Batch 1200, Loss: 0.1142\n",
            "  Batch 1300, Loss: 0.1702\n",
            "  Batch 1400, Loss: 0.2414\n",
            "  Batch 1500, Loss: 0.1772\n",
            "  Batch 1600, Loss: 0.2327\n",
            "  Batch 1700, Loss: 0.2367\n",
            "  Batch 1800, Loss: 0.1929\n",
            "Epoch 63 completed with average loss: 0.2295\n",
            "\n",
            "epoch64/100\n",
            "  Batch 0, Loss: 0.1605\n",
            "  Batch 100, Loss: 0.2331\n",
            "  Batch 200, Loss: 0.1414\n",
            "  Batch 300, Loss: 0.2940\n",
            "  Batch 400, Loss: 0.1901\n",
            "  Batch 500, Loss: 0.1338\n",
            "  Batch 600, Loss: 0.1572\n",
            "  Batch 700, Loss: 0.1937\n",
            "  Batch 800, Loss: 0.1521\n",
            "  Batch 900, Loss: 0.1702\n",
            "  Batch 1000, Loss: 0.3142\n",
            "  Batch 1100, Loss: 0.3688\n",
            "  Batch 1200, Loss: 0.3138\n",
            "  Batch 1300, Loss: 0.1887\n",
            "  Batch 1400, Loss: 0.2986\n",
            "  Batch 1500, Loss: 0.0972\n",
            "  Batch 1600, Loss: 0.2267\n",
            "  Batch 1700, Loss: 0.2444\n",
            "  Batch 1800, Loss: 0.4863\n",
            "Epoch 64 completed with average loss: 0.2272\n",
            "\n",
            "epoch65/100\n",
            "  Batch 0, Loss: 0.3214\n",
            "  Batch 100, Loss: 0.2266\n",
            "  Batch 200, Loss: 0.1339\n",
            "  Batch 300, Loss: 0.2105\n",
            "  Batch 400, Loss: 0.2718\n",
            "  Batch 500, Loss: 0.2331\n",
            "  Batch 600, Loss: 0.1417\n",
            "  Batch 700, Loss: 0.2434\n",
            "  Batch 800, Loss: 0.2507\n",
            "  Batch 900, Loss: 0.1235\n",
            "  Batch 1000, Loss: 0.1743\n",
            "  Batch 1100, Loss: 0.2114\n",
            "  Batch 1200, Loss: 0.2056\n",
            "  Batch 1300, Loss: 0.1466\n",
            "  Batch 1400, Loss: 0.1724\n",
            "  Batch 1500, Loss: 0.3665\n",
            "  Batch 1600, Loss: 0.4000\n",
            "  Batch 1700, Loss: 0.3719\n",
            "  Batch 1800, Loss: 0.1679\n",
            "Epoch 65 completed with average loss: 0.2260\n",
            "\n",
            "epoch66/100\n",
            "  Batch 0, Loss: 0.1411\n",
            "  Batch 100, Loss: 0.2365\n",
            "  Batch 200, Loss: 0.1688\n",
            "  Batch 300, Loss: 0.1258\n",
            "  Batch 400, Loss: 0.1041\n",
            "  Batch 500, Loss: 0.2631\n",
            "  Batch 600, Loss: 0.3016\n",
            "  Batch 700, Loss: 0.1308\n",
            "  Batch 800, Loss: 0.1794\n",
            "  Batch 900, Loss: 0.2102\n",
            "  Batch 1000, Loss: 0.2434\n",
            "  Batch 1100, Loss: 0.1579\n",
            "  Batch 1200, Loss: 0.1698\n",
            "  Batch 1300, Loss: 0.1753\n",
            "  Batch 1400, Loss: 0.2188\n",
            "  Batch 1500, Loss: 0.1981\n",
            "  Batch 1600, Loss: 0.2670\n",
            "  Batch 1700, Loss: 0.2520\n",
            "  Batch 1800, Loss: 0.1395\n",
            "Epoch 66 completed with average loss: 0.2241\n",
            "\n",
            "epoch67/100\n",
            "  Batch 0, Loss: 0.3316\n",
            "  Batch 100, Loss: 0.2774\n",
            "  Batch 200, Loss: 0.3477\n",
            "  Batch 300, Loss: 0.3530\n",
            "  Batch 400, Loss: 0.0833\n",
            "  Batch 500, Loss: 0.1926\n",
            "  Batch 600, Loss: 0.5010\n",
            "  Batch 700, Loss: 0.2202\n",
            "  Batch 800, Loss: 0.1111\n",
            "  Batch 900, Loss: 0.2120\n",
            "  Batch 1000, Loss: 0.0448\n",
            "  Batch 1100, Loss: 0.3792\n",
            "  Batch 1200, Loss: 0.2908\n",
            "  Batch 1300, Loss: 0.1130\n",
            "  Batch 1400, Loss: 0.1666\n",
            "  Batch 1500, Loss: 0.2485\n",
            "  Batch 1600, Loss: 0.1074\n",
            "  Batch 1700, Loss: 0.2040\n",
            "  Batch 1800, Loss: 0.2261\n",
            "Epoch 67 completed with average loss: 0.2221\n",
            "\n",
            "epoch68/100\n",
            "  Batch 0, Loss: 0.0885\n",
            "  Batch 100, Loss: 0.1507\n",
            "  Batch 200, Loss: 0.2474\n",
            "  Batch 300, Loss: 0.1775\n",
            "  Batch 400, Loss: 0.1593\n",
            "  Batch 500, Loss: 0.1583\n",
            "  Batch 600, Loss: 0.4496\n",
            "  Batch 700, Loss: 0.2513\n",
            "  Batch 800, Loss: 0.0881\n",
            "  Batch 900, Loss: 0.1963\n",
            "  Batch 1000, Loss: 0.3614\n",
            "  Batch 1100, Loss: 0.0970\n",
            "  Batch 1200, Loss: 0.1083\n",
            "  Batch 1300, Loss: 0.3493\n",
            "  Batch 1400, Loss: 0.3310\n",
            "  Batch 1500, Loss: 0.2069\n",
            "  Batch 1600, Loss: 0.1870\n",
            "  Batch 1700, Loss: 0.1857\n",
            "  Batch 1800, Loss: 0.1840\n",
            "Epoch 68 completed with average loss: 0.2213\n",
            "\n",
            "epoch69/100\n",
            "  Batch 0, Loss: 0.3357\n",
            "  Batch 100, Loss: 0.2705\n",
            "  Batch 200, Loss: 0.1286\n",
            "  Batch 300, Loss: 0.2299\n",
            "  Batch 400, Loss: 0.2463\n",
            "  Batch 500, Loss: 0.3953\n",
            "  Batch 600, Loss: 0.1004\n",
            "  Batch 700, Loss: 0.3155\n",
            "  Batch 800, Loss: 0.2074\n",
            "  Batch 900, Loss: 0.1695\n",
            "  Batch 1000, Loss: 0.1564\n",
            "  Batch 1100, Loss: 0.1035\n",
            "  Batch 1200, Loss: 0.3640\n",
            "  Batch 1300, Loss: 0.2367\n",
            "  Batch 1400, Loss: 0.0975\n",
            "  Batch 1500, Loss: 0.1219\n",
            "  Batch 1600, Loss: 0.2579\n",
            "  Batch 1700, Loss: 0.3249\n",
            "  Batch 1800, Loss: 0.1240\n",
            "Epoch 69 completed with average loss: 0.2195\n",
            "\n",
            "epoch70/100\n",
            "  Batch 0, Loss: 0.3283\n",
            "  Batch 100, Loss: 0.2749\n",
            "  Batch 200, Loss: 0.0857\n",
            "  Batch 300, Loss: 0.1505\n",
            "  Batch 400, Loss: 0.2197\n",
            "  Batch 500, Loss: 0.1499\n",
            "  Batch 600, Loss: 0.1533\n",
            "  Batch 700, Loss: 0.1984\n",
            "  Batch 800, Loss: 0.1757\n",
            "  Batch 900, Loss: 0.2758\n",
            "  Batch 1000, Loss: 0.3098\n",
            "  Batch 1100, Loss: 0.1649\n",
            "  Batch 1200, Loss: 0.1067\n",
            "  Batch 1300, Loss: 0.2611\n",
            "  Batch 1400, Loss: 0.2981\n",
            "  Batch 1500, Loss: 0.3036\n",
            "  Batch 1600, Loss: 0.1937\n",
            "  Batch 1700, Loss: 0.2843\n",
            "  Batch 1800, Loss: 0.1708\n",
            "Epoch 70 completed with average loss: 0.2185\n",
            "\n",
            "epoch71/100\n",
            "  Batch 0, Loss: 0.1507\n",
            "  Batch 100, Loss: 0.1468\n",
            "  Batch 200, Loss: 0.2299\n",
            "  Batch 300, Loss: 0.1558\n",
            "  Batch 400, Loss: 0.2615\n",
            "  Batch 500, Loss: 0.4784\n",
            "  Batch 600, Loss: 0.1081\n",
            "  Batch 700, Loss: 0.3224\n",
            "  Batch 800, Loss: 0.1129\n",
            "  Batch 900, Loss: 0.1677\n",
            "  Batch 1000, Loss: 0.1847\n",
            "  Batch 1100, Loss: 0.3992\n",
            "  Batch 1200, Loss: 0.1691\n",
            "  Batch 1300, Loss: 0.1791\n",
            "  Batch 1400, Loss: 0.2829\n",
            "  Batch 1500, Loss: 0.2203\n",
            "  Batch 1600, Loss: 0.0887\n",
            "  Batch 1700, Loss: 0.4603\n",
            "  Batch 1800, Loss: 0.1500\n",
            "Epoch 71 completed with average loss: 0.2172\n",
            "\n",
            "epoch72/100\n",
            "  Batch 0, Loss: 0.3015\n",
            "  Batch 100, Loss: 0.0984\n",
            "  Batch 200, Loss: 0.1795\n",
            "  Batch 300, Loss: 0.0759\n",
            "  Batch 400, Loss: 0.1663\n",
            "  Batch 500, Loss: 0.3257\n",
            "  Batch 600, Loss: 0.1953\n",
            "  Batch 700, Loss: 0.1500\n",
            "  Batch 800, Loss: 0.1379\n",
            "  Batch 900, Loss: 0.2983\n",
            "  Batch 1000, Loss: 0.1144\n",
            "  Batch 1100, Loss: 0.3203\n",
            "  Batch 1200, Loss: 0.3828\n",
            "  Batch 1300, Loss: 0.1844\n",
            "  Batch 1400, Loss: 0.1073\n",
            "  Batch 1500, Loss: 0.3042\n",
            "  Batch 1600, Loss: 0.2869\n",
            "  Batch 1700, Loss: 0.3888\n",
            "  Batch 1800, Loss: 0.2875\n",
            "Epoch 72 completed with average loss: 0.2156\n",
            "\n",
            "epoch73/100\n",
            "  Batch 0, Loss: 0.1913\n",
            "  Batch 100, Loss: 0.1782\n",
            "  Batch 200, Loss: 0.1441\n",
            "  Batch 300, Loss: 0.2069\n",
            "  Batch 400, Loss: 0.1150\n",
            "  Batch 500, Loss: 0.1924\n",
            "  Batch 600, Loss: 0.2772\n",
            "  Batch 700, Loss: 0.3056\n",
            "  Batch 800, Loss: 0.2609\n",
            "  Batch 900, Loss: 0.1173\n",
            "  Batch 1000, Loss: 0.3018\n",
            "  Batch 1100, Loss: 0.2303\n",
            "  Batch 1200, Loss: 0.2358\n",
            "  Batch 1300, Loss: 0.0954\n",
            "  Batch 1400, Loss: 0.2203\n",
            "  Batch 1500, Loss: 0.3024\n",
            "  Batch 1600, Loss: 0.1581\n",
            "  Batch 1700, Loss: 0.1687\n",
            "  Batch 1800, Loss: 0.2039\n",
            "Epoch 73 completed with average loss: 0.2131\n",
            "\n",
            "epoch74/100\n",
            "  Batch 0, Loss: 0.4120\n",
            "  Batch 100, Loss: 0.4196\n",
            "  Batch 200, Loss: 0.3005\n",
            "  Batch 300, Loss: 0.3196\n",
            "  Batch 400, Loss: 0.1684\n",
            "  Batch 500, Loss: 0.3123\n",
            "  Batch 600, Loss: 0.1194\n",
            "  Batch 700, Loss: 0.0835\n",
            "  Batch 800, Loss: 0.0919\n",
            "  Batch 900, Loss: 0.2655\n",
            "  Batch 1000, Loss: 0.2821\n",
            "  Batch 1100, Loss: 0.0948\n",
            "  Batch 1200, Loss: 0.1373\n",
            "  Batch 1300, Loss: 0.1157\n",
            "  Batch 1400, Loss: 0.1226\n",
            "  Batch 1500, Loss: 0.2326\n",
            "  Batch 1600, Loss: 0.4754\n",
            "  Batch 1700, Loss: 0.2477\n",
            "  Batch 1800, Loss: 0.1682\n",
            "Epoch 74 completed with average loss: 0.2128\n",
            "\n",
            "epoch75/100\n",
            "  Batch 0, Loss: 0.3205\n",
            "  Batch 100, Loss: 0.2164\n",
            "  Batch 200, Loss: 0.0831\n",
            "  Batch 300, Loss: 0.2384\n",
            "  Batch 400, Loss: 0.1037\n",
            "  Batch 500, Loss: 0.2005\n",
            "  Batch 600, Loss: 0.0786\n",
            "  Batch 700, Loss: 0.1386\n",
            "  Batch 800, Loss: 0.1157\n",
            "  Batch 900, Loss: 0.1079\n",
            "  Batch 1000, Loss: 0.1757\n",
            "  Batch 1100, Loss: 0.3802\n",
            "  Batch 1200, Loss: 0.1300\n",
            "  Batch 1300, Loss: 0.2841\n",
            "  Batch 1400, Loss: 0.1395\n",
            "  Batch 1500, Loss: 0.0967\n",
            "  Batch 1600, Loss: 0.1666\n",
            "  Batch 1700, Loss: 0.2237\n",
            "  Batch 1800, Loss: 0.1459\n",
            "Epoch 75 completed with average loss: 0.2112\n",
            "\n",
            "epoch76/100\n",
            "  Batch 0, Loss: 0.1781\n",
            "  Batch 100, Loss: 0.1498\n",
            "  Batch 200, Loss: 0.2466\n",
            "  Batch 300, Loss: 0.4454\n",
            "  Batch 400, Loss: 0.1772\n",
            "  Batch 500, Loss: 0.3870\n",
            "  Batch 600, Loss: 0.3533\n",
            "  Batch 700, Loss: 0.1536\n",
            "  Batch 800, Loss: 0.2777\n",
            "  Batch 900, Loss: 0.2310\n",
            "  Batch 1000, Loss: 0.2907\n",
            "  Batch 1100, Loss: 0.1389\n",
            "  Batch 1200, Loss: 0.1353\n",
            "  Batch 1300, Loss: 0.1950\n",
            "  Batch 1400, Loss: 0.0304\n",
            "  Batch 1500, Loss: 0.2671\n",
            "  Batch 1600, Loss: 0.1336\n",
            "  Batch 1700, Loss: 0.2577\n",
            "  Batch 1800, Loss: 0.2674\n",
            "Epoch 76 completed with average loss: 0.2103\n",
            "\n",
            "epoch77/100\n",
            "  Batch 0, Loss: 0.0552\n",
            "  Batch 100, Loss: 0.3255\n",
            "  Batch 200, Loss: 0.1664\n",
            "  Batch 300, Loss: 0.2138\n",
            "  Batch 400, Loss: 0.5552\n",
            "  Batch 500, Loss: 0.1926\n",
            "  Batch 600, Loss: 0.2457\n",
            "  Batch 700, Loss: 0.0697\n",
            "  Batch 800, Loss: 0.1627\n",
            "  Batch 900, Loss: 0.2242\n",
            "  Batch 1000, Loss: 0.2239\n",
            "  Batch 1100, Loss: 0.3315\n",
            "  Batch 1200, Loss: 0.1793\n",
            "  Batch 1300, Loss: 0.2031\n",
            "  Batch 1400, Loss: 0.2182\n",
            "  Batch 1500, Loss: 0.3654\n",
            "  Batch 1600, Loss: 0.2275\n",
            "  Batch 1700, Loss: 0.1982\n",
            "  Batch 1800, Loss: 0.0995\n",
            "Epoch 77 completed with average loss: 0.2079\n",
            "\n",
            "epoch78/100\n",
            "  Batch 0, Loss: 0.1127\n",
            "  Batch 100, Loss: 0.1278\n",
            "  Batch 200, Loss: 0.1449\n",
            "  Batch 300, Loss: 0.1551\n",
            "  Batch 400, Loss: 0.2147\n",
            "  Batch 500, Loss: 0.3685\n",
            "  Batch 600, Loss: 0.1746\n",
            "  Batch 700, Loss: 0.2714\n",
            "  Batch 800, Loss: 0.1806\n",
            "  Batch 900, Loss: 0.1769\n",
            "  Batch 1000, Loss: 0.1990\n",
            "  Batch 1100, Loss: 0.2455\n",
            "  Batch 1200, Loss: 0.1027\n",
            "  Batch 1300, Loss: 0.2549\n",
            "  Batch 1400, Loss: 0.1710\n",
            "  Batch 1500, Loss: 0.2110\n",
            "  Batch 1600, Loss: 0.2442\n",
            "  Batch 1700, Loss: 0.1972\n",
            "  Batch 1800, Loss: 0.0995\n",
            "Epoch 78 completed with average loss: 0.2072\n",
            "\n",
            "epoch79/100\n",
            "  Batch 0, Loss: 0.3842\n",
            "  Batch 100, Loss: 0.0957\n",
            "  Batch 200, Loss: 0.1966\n",
            "  Batch 300, Loss: 0.1910\n",
            "  Batch 400, Loss: 0.0373\n",
            "  Batch 500, Loss: 0.4021\n",
            "  Batch 600, Loss: 0.3538\n",
            "  Batch 700, Loss: 0.0778\n",
            "  Batch 800, Loss: 0.1428\n",
            "  Batch 900, Loss: 0.2014\n",
            "  Batch 1000, Loss: 0.1702\n",
            "  Batch 1100, Loss: 0.2034\n",
            "  Batch 1200, Loss: 0.0877\n",
            "  Batch 1300, Loss: 0.1846\n",
            "  Batch 1400, Loss: 0.2806\n",
            "  Batch 1500, Loss: 0.2243\n",
            "  Batch 1600, Loss: 0.2365\n",
            "  Batch 1700, Loss: 0.1408\n",
            "  Batch 1800, Loss: 0.1839\n",
            "Epoch 79 completed with average loss: 0.2063\n",
            "\n",
            "epoch80/100\n",
            "  Batch 0, Loss: 0.1587\n",
            "  Batch 100, Loss: 0.2512\n",
            "  Batch 200, Loss: 0.0807\n",
            "  Batch 300, Loss: 0.1250\n",
            "  Batch 400, Loss: 0.1107\n",
            "  Batch 500, Loss: 0.1618\n",
            "  Batch 600, Loss: 0.1245\n",
            "  Batch 700, Loss: 0.1558\n",
            "  Batch 800, Loss: 0.4021\n",
            "  Batch 900, Loss: 0.1563\n",
            "  Batch 1000, Loss: 0.4231\n",
            "  Batch 1100, Loss: 0.1893\n",
            "  Batch 1200, Loss: 0.3830\n",
            "  Batch 1300, Loss: 0.2422\n",
            "  Batch 1400, Loss: 0.2036\n",
            "  Batch 1500, Loss: 0.0748\n",
            "  Batch 1600, Loss: 0.1503\n",
            "  Batch 1700, Loss: 0.2437\n",
            "  Batch 1800, Loss: 0.1674\n",
            "Epoch 80 completed with average loss: 0.2041\n",
            "\n",
            "epoch81/100\n",
            "  Batch 0, Loss: 0.1441\n",
            "  Batch 100, Loss: 0.4265\n",
            "  Batch 200, Loss: 0.1656\n",
            "  Batch 300, Loss: 0.0804\n",
            "  Batch 400, Loss: 0.1287\n",
            "  Batch 500, Loss: 0.1512\n",
            "  Batch 600, Loss: 0.0610\n",
            "  Batch 700, Loss: 0.1593\n",
            "  Batch 800, Loss: 0.6817\n",
            "  Batch 900, Loss: 0.1567\n",
            "  Batch 1000, Loss: 0.1207\n",
            "  Batch 1100, Loss: 0.2212\n",
            "  Batch 1200, Loss: 0.0957\n",
            "  Batch 1300, Loss: 0.1123\n",
            "  Batch 1400, Loss: 0.1816\n",
            "  Batch 1500, Loss: 0.0706\n",
            "  Batch 1600, Loss: 0.0888\n",
            "  Batch 1700, Loss: 0.1652\n",
            "  Batch 1800, Loss: 0.2377\n",
            "Epoch 81 completed with average loss: 0.2030\n",
            "\n",
            "epoch82/100\n",
            "  Batch 0, Loss: 0.2879\n",
            "  Batch 100, Loss: 0.0988\n",
            "  Batch 200, Loss: 0.2449\n",
            "  Batch 300, Loss: 0.4173\n",
            "  Batch 400, Loss: 0.1410\n",
            "  Batch 500, Loss: 0.1086\n",
            "  Batch 600, Loss: 0.1115\n",
            "  Batch 700, Loss: 0.2368\n",
            "  Batch 800, Loss: 0.1054\n",
            "  Batch 900, Loss: 0.3214\n",
            "  Batch 1000, Loss: 0.0541\n",
            "  Batch 1100, Loss: 0.1275\n",
            "  Batch 1200, Loss: 0.1123\n",
            "  Batch 1300, Loss: 0.3031\n",
            "  Batch 1400, Loss: 0.1500\n",
            "  Batch 1500, Loss: 0.3312\n",
            "  Batch 1600, Loss: 0.2062\n",
            "  Batch 1700, Loss: 0.2618\n",
            "  Batch 1800, Loss: 0.2428\n",
            "Epoch 82 completed with average loss: 0.2016\n",
            "\n",
            "epoch83/100\n",
            "  Batch 0, Loss: 0.1884\n",
            "  Batch 100, Loss: 0.1673\n",
            "  Batch 200, Loss: 0.1495\n",
            "  Batch 300, Loss: 0.1142\n",
            "  Batch 400, Loss: 0.3097\n",
            "  Batch 500, Loss: 0.3639\n",
            "  Batch 600, Loss: 0.0551\n",
            "  Batch 700, Loss: 0.1802\n",
            "  Batch 800, Loss: 0.1445\n",
            "  Batch 900, Loss: 0.1897\n",
            "  Batch 1000, Loss: 0.1466\n",
            "  Batch 1100, Loss: 0.3937\n",
            "  Batch 1200, Loss: 0.1657\n",
            "  Batch 1300, Loss: 0.3317\n",
            "  Batch 1400, Loss: 0.3911\n",
            "  Batch 1500, Loss: 0.1849\n",
            "  Batch 1600, Loss: 0.0959\n",
            "  Batch 1700, Loss: 0.2075\n",
            "  Batch 1800, Loss: 0.1026\n",
            "Epoch 83 completed with average loss: 0.1995\n",
            "\n",
            "epoch84/100\n",
            "  Batch 0, Loss: 0.2060\n",
            "  Batch 100, Loss: 0.1153\n",
            "  Batch 200, Loss: 0.1764\n",
            "  Batch 300, Loss: 0.4702\n",
            "  Batch 400, Loss: 0.1857\n",
            "  Batch 500, Loss: 0.1336\n",
            "  Batch 600, Loss: 0.2867\n",
            "  Batch 700, Loss: 0.1578\n",
            "  Batch 800, Loss: 0.2150\n",
            "  Batch 900, Loss: 0.2266\n",
            "  Batch 1000, Loss: 0.2098\n",
            "  Batch 1100, Loss: 0.1828\n",
            "  Batch 1200, Loss: 0.1862\n",
            "  Batch 1300, Loss: 0.2639\n",
            "  Batch 1400, Loss: 0.1622\n",
            "  Batch 1500, Loss: 0.1635\n",
            "  Batch 1600, Loss: 0.3868\n",
            "  Batch 1700, Loss: 0.3120\n",
            "  Batch 1800, Loss: 0.1784\n",
            "Epoch 84 completed with average loss: 0.1992\n",
            "\n",
            "epoch85/100\n",
            "  Batch 0, Loss: 0.2881\n",
            "  Batch 100, Loss: 0.1869\n",
            "  Batch 200, Loss: 0.0688\n",
            "  Batch 300, Loss: 0.2581\n",
            "  Batch 400, Loss: 0.1095\n",
            "  Batch 500, Loss: 0.1895\n",
            "  Batch 600, Loss: 0.1680\n",
            "  Batch 700, Loss: 0.3339\n",
            "  Batch 800, Loss: 0.0843\n",
            "  Batch 900, Loss: 0.1195\n",
            "  Batch 1000, Loss: 0.1539\n",
            "  Batch 1100, Loss: 0.1619\n",
            "  Batch 1200, Loss: 0.2486\n",
            "  Batch 1300, Loss: 0.4311\n",
            "  Batch 1400, Loss: 0.1161\n",
            "  Batch 1500, Loss: 0.2097\n",
            "  Batch 1600, Loss: 0.1499\n",
            "  Batch 1700, Loss: 0.1271\n",
            "  Batch 1800, Loss: 0.1971\n",
            "Epoch 85 completed with average loss: 0.1975\n",
            "\n",
            "epoch86/100\n",
            "  Batch 0, Loss: 0.1097\n",
            "  Batch 100, Loss: 0.1776\n",
            "  Batch 200, Loss: 0.1179\n",
            "  Batch 300, Loss: 0.2122\n",
            "  Batch 400, Loss: 0.1990\n",
            "  Batch 500, Loss: 0.3960\n",
            "  Batch 600, Loss: 0.1548\n",
            "  Batch 700, Loss: 0.1770\n",
            "  Batch 800, Loss: 0.2584\n",
            "  Batch 900, Loss: 0.2669\n",
            "  Batch 1000, Loss: 0.2151\n",
            "  Batch 1100, Loss: 0.2513\n",
            "  Batch 1200, Loss: 0.1603\n",
            "  Batch 1300, Loss: 0.2803\n",
            "  Batch 1400, Loss: 0.1613\n",
            "  Batch 1500, Loss: 0.1290\n",
            "  Batch 1600, Loss: 0.2194\n",
            "  Batch 1700, Loss: 0.1798\n",
            "  Batch 1800, Loss: 0.1304\n",
            "Epoch 86 completed with average loss: 0.1960\n",
            "\n",
            "epoch87/100\n",
            "  Batch 0, Loss: 0.1465\n",
            "  Batch 100, Loss: 0.2224\n",
            "  Batch 200, Loss: 0.3085\n",
            "  Batch 300, Loss: 0.1293\n",
            "  Batch 400, Loss: 0.3079\n",
            "  Batch 500, Loss: 0.0842\n",
            "  Batch 600, Loss: 0.0937\n",
            "  Batch 700, Loss: 0.2847\n",
            "  Batch 800, Loss: 0.1595\n",
            "  Batch 900, Loss: 0.1063\n",
            "  Batch 1000, Loss: 0.0805\n",
            "  Batch 1100, Loss: 0.1153\n",
            "  Batch 1200, Loss: 0.2078\n",
            "  Batch 1300, Loss: 0.1090\n",
            "  Batch 1400, Loss: 0.1466\n",
            "  Batch 1500, Loss: 0.1870\n",
            "  Batch 1600, Loss: 0.0516\n",
            "  Batch 1700, Loss: 0.1684\n",
            "  Batch 1800, Loss: 0.1394\n",
            "Epoch 87 completed with average loss: 0.1950\n",
            "\n",
            "epoch88/100\n",
            "  Batch 0, Loss: 0.3140\n",
            "  Batch 100, Loss: 0.2328\n",
            "  Batch 200, Loss: 0.1000\n",
            "  Batch 300, Loss: 0.1895\n",
            "  Batch 400, Loss: 0.0667\n",
            "  Batch 500, Loss: 0.2300\n",
            "  Batch 600, Loss: 0.1740\n",
            "  Batch 700, Loss: 0.1575\n",
            "  Batch 800, Loss: 0.0919\n",
            "  Batch 900, Loss: 0.2615\n",
            "  Batch 1000, Loss: 0.1503\n",
            "  Batch 1100, Loss: 0.2552\n",
            "  Batch 1200, Loss: 0.0262\n",
            "  Batch 1300, Loss: 0.1360\n",
            "  Batch 1400, Loss: 0.1118\n",
            "  Batch 1500, Loss: 0.1592\n",
            "  Batch 1600, Loss: 0.4769\n",
            "  Batch 1700, Loss: 0.1449\n",
            "  Batch 1800, Loss: 0.1404\n",
            "Epoch 88 completed with average loss: 0.1943\n",
            "\n",
            "epoch89/100\n",
            "  Batch 0, Loss: 0.0640\n",
            "  Batch 100, Loss: 0.6722\n",
            "  Batch 200, Loss: 0.1409\n",
            "  Batch 300, Loss: 0.0762\n",
            "  Batch 400, Loss: 0.2861\n",
            "  Batch 500, Loss: 0.1679\n",
            "  Batch 600, Loss: 0.1612\n",
            "  Batch 700, Loss: 0.1171\n",
            "  Batch 800, Loss: 0.1885\n",
            "  Batch 900, Loss: 0.1204\n",
            "  Batch 1000, Loss: 0.0797\n",
            "  Batch 1100, Loss: 0.2104\n",
            "  Batch 1200, Loss: 0.0846\n",
            "  Batch 1300, Loss: 0.3770\n",
            "  Batch 1400, Loss: 0.0833\n",
            "  Batch 1500, Loss: 0.2970\n",
            "  Batch 1600, Loss: 0.1637\n",
            "  Batch 1700, Loss: 0.2040\n",
            "  Batch 1800, Loss: 0.2815\n",
            "Epoch 89 completed with average loss: 0.1927\n",
            "\n",
            "epoch90/100\n",
            "  Batch 0, Loss: 0.1365\n",
            "  Batch 100, Loss: 0.2193\n",
            "  Batch 200, Loss: 0.1686\n",
            "  Batch 300, Loss: 0.1590\n",
            "  Batch 400, Loss: 0.1095\n",
            "  Batch 500, Loss: 0.2391\n",
            "  Batch 600, Loss: 0.1668\n",
            "  Batch 700, Loss: 0.2297\n",
            "  Batch 800, Loss: 0.2442\n",
            "  Batch 900, Loss: 0.2572\n",
            "  Batch 1000, Loss: 0.3463\n",
            "  Batch 1100, Loss: 0.1877\n",
            "  Batch 1200, Loss: 0.2838\n",
            "  Batch 1300, Loss: 0.0937\n",
            "  Batch 1400, Loss: 0.1628\n",
            "  Batch 1500, Loss: 0.0886\n",
            "  Batch 1600, Loss: 0.1648\n",
            "  Batch 1700, Loss: 0.3311\n",
            "  Batch 1800, Loss: 0.1778\n",
            "Epoch 90 completed with average loss: 0.1914\n",
            "\n",
            "epoch91/100\n",
            "  Batch 0, Loss: 0.2362\n",
            "  Batch 100, Loss: 0.2236\n",
            "  Batch 200, Loss: 0.1985\n",
            "  Batch 300, Loss: 0.2574\n",
            "  Batch 400, Loss: 0.2149\n",
            "  Batch 500, Loss: 0.0721\n",
            "  Batch 600, Loss: 0.1601\n",
            "  Batch 700, Loss: 0.2582\n",
            "  Batch 800, Loss: 0.2666\n",
            "  Batch 900, Loss: 0.1294\n",
            "  Batch 1000, Loss: 0.0728\n",
            "  Batch 1100, Loss: 0.1890\n",
            "  Batch 1200, Loss: 0.2554\n",
            "  Batch 1300, Loss: 0.3274\n",
            "  Batch 1400, Loss: 0.0767\n",
            "  Batch 1500, Loss: 0.1730\n",
            "  Batch 1600, Loss: 0.1437\n",
            "  Batch 1700, Loss: 0.2976\n",
            "  Batch 1800, Loss: 0.1278\n",
            "Epoch 91 completed with average loss: 0.1906\n",
            "\n",
            "epoch92/100\n",
            "  Batch 0, Loss: 0.2697\n",
            "  Batch 100, Loss: 0.2772\n",
            "  Batch 200, Loss: 0.1679\n",
            "  Batch 300, Loss: 0.0854\n",
            "  Batch 400, Loss: 0.1976\n",
            "  Batch 500, Loss: 0.0478\n",
            "  Batch 600, Loss: 0.0505\n",
            "  Batch 700, Loss: 0.1491\n",
            "  Batch 800, Loss: 0.2094\n",
            "  Batch 900, Loss: 0.2178\n",
            "  Batch 1000, Loss: 0.1384\n",
            "  Batch 1100, Loss: 0.1626\n",
            "  Batch 1200, Loss: 0.1531\n",
            "  Batch 1300, Loss: 0.1711\n",
            "  Batch 1400, Loss: 0.0701\n",
            "  Batch 1500, Loss: 0.1725\n",
            "  Batch 1600, Loss: 0.1509\n",
            "  Batch 1700, Loss: 0.1289\n",
            "  Batch 1800, Loss: 0.1446\n",
            "Epoch 92 completed with average loss: 0.1897\n",
            "\n",
            "epoch93/100\n",
            "  Batch 0, Loss: 0.1823\n",
            "  Batch 100, Loss: 0.1127\n",
            "  Batch 200, Loss: 0.1081\n",
            "  Batch 300, Loss: 0.2539\n",
            "  Batch 400, Loss: 0.1136\n",
            "  Batch 500, Loss: 0.2485\n",
            "  Batch 600, Loss: 0.1226\n",
            "  Batch 700, Loss: 0.2349\n",
            "  Batch 800, Loss: 0.4537\n",
            "  Batch 900, Loss: 0.1962\n",
            "  Batch 1000, Loss: 0.0877\n",
            "  Batch 1100, Loss: 0.2782\n",
            "  Batch 1200, Loss: 0.2955\n",
            "  Batch 1300, Loss: 0.3297\n",
            "  Batch 1400, Loss: 0.3257\n",
            "  Batch 1500, Loss: 0.1600\n",
            "  Batch 1600, Loss: 0.3660\n",
            "  Batch 1700, Loss: 0.1482\n",
            "  Batch 1800, Loss: 0.1468\n",
            "Epoch 93 completed with average loss: 0.1874\n",
            "\n",
            "epoch94/100\n",
            "  Batch 0, Loss: 0.1732\n",
            "  Batch 100, Loss: 0.1810\n",
            "  Batch 200, Loss: 0.1579\n",
            "  Batch 300, Loss: 0.1459\n",
            "  Batch 400, Loss: 0.1484\n",
            "  Batch 500, Loss: 0.3351\n",
            "  Batch 600, Loss: 0.1100\n",
            "  Batch 700, Loss: 0.1904\n",
            "  Batch 800, Loss: 0.1618\n",
            "  Batch 900, Loss: 0.3022\n",
            "  Batch 1000, Loss: 0.1606\n",
            "  Batch 1100, Loss: 0.1302\n",
            "  Batch 1200, Loss: 0.0858\n",
            "  Batch 1300, Loss: 0.1883\n",
            "  Batch 1400, Loss: 0.3061\n",
            "  Batch 1500, Loss: 0.1693\n",
            "  Batch 1600, Loss: 0.1330\n",
            "  Batch 1700, Loss: 0.2851\n",
            "  Batch 1800, Loss: 0.1622\n",
            "Epoch 94 completed with average loss: 0.1870\n",
            "\n",
            "epoch95/100\n",
            "  Batch 0, Loss: 0.0754\n",
            "  Batch 100, Loss: 0.1882\n",
            "  Batch 200, Loss: 0.1449\n",
            "  Batch 300, Loss: 0.1950\n",
            "  Batch 400, Loss: 0.0798\n",
            "  Batch 500, Loss: 0.1380\n",
            "  Batch 600, Loss: 0.1400\n",
            "  Batch 700, Loss: 0.1218\n",
            "  Batch 800, Loss: 0.2790\n",
            "  Batch 900, Loss: 0.1589\n",
            "  Batch 1000, Loss: 0.1554\n",
            "  Batch 1100, Loss: 0.3209\n",
            "  Batch 1200, Loss: 0.1118\n",
            "  Batch 1300, Loss: 0.4553\n",
            "  Batch 1400, Loss: 0.0766\n",
            "  Batch 1500, Loss: 0.2066\n",
            "  Batch 1600, Loss: 0.1494\n",
            "  Batch 1700, Loss: 0.1375\n",
            "  Batch 1800, Loss: 0.2322\n",
            "Epoch 95 completed with average loss: 0.1854\n",
            "\n",
            "epoch96/100\n",
            "  Batch 0, Loss: 0.3819\n",
            "  Batch 100, Loss: 0.0751\n",
            "  Batch 200, Loss: 0.1330\n",
            "  Batch 300, Loss: 0.1683\n",
            "  Batch 400, Loss: 0.1637\n",
            "  Batch 500, Loss: 0.4002\n",
            "  Batch 600, Loss: 0.0405\n",
            "  Batch 700, Loss: 0.1750\n",
            "  Batch 800, Loss: 0.0536\n",
            "  Batch 900, Loss: 0.2553\n",
            "  Batch 1000, Loss: 0.1689\n",
            "  Batch 1100, Loss: 0.0674\n",
            "  Batch 1200, Loss: 0.2030\n",
            "  Batch 1300, Loss: 0.2834\n",
            "  Batch 1400, Loss: 0.1210\n",
            "  Batch 1500, Loss: 0.0803\n",
            "  Batch 1600, Loss: 0.1277\n",
            "  Batch 1700, Loss: 0.1386\n",
            "  Batch 1800, Loss: 0.2362\n",
            "Epoch 96 completed with average loss: 0.1845\n",
            "\n",
            "epoch97/100\n",
            "  Batch 0, Loss: 0.2473\n",
            "  Batch 100, Loss: 0.2199\n",
            "  Batch 200, Loss: 0.1904\n",
            "  Batch 300, Loss: 0.0747\n",
            "  Batch 400, Loss: 0.1022\n",
            "  Batch 500, Loss: 0.1646\n",
            "  Batch 600, Loss: 0.0829\n",
            "  Batch 700, Loss: 0.3338\n",
            "  Batch 800, Loss: 0.2130\n",
            "  Batch 900, Loss: 0.2237\n",
            "  Batch 1000, Loss: 0.1761\n",
            "  Batch 1100, Loss: 0.1772\n",
            "  Batch 1200, Loss: 0.3071\n",
            "  Batch 1300, Loss: 0.1382\n",
            "  Batch 1400, Loss: 0.1898\n",
            "  Batch 1500, Loss: 0.1668\n",
            "  Batch 1600, Loss: 0.1756\n",
            "  Batch 1700, Loss: 0.3095\n",
            "  Batch 1800, Loss: 0.2585\n",
            "Epoch 97 completed with average loss: 0.1828\n",
            "\n",
            "epoch98/100\n",
            "  Batch 0, Loss: 0.0634\n",
            "  Batch 100, Loss: 0.1164\n",
            "  Batch 200, Loss: 0.2407\n",
            "  Batch 300, Loss: 0.0742\n",
            "  Batch 400, Loss: 0.2087\n",
            "  Batch 500, Loss: 0.2096\n",
            "  Batch 600, Loss: 0.1725\n",
            "  Batch 700, Loss: 0.1092\n",
            "  Batch 800, Loss: 0.1834\n",
            "  Batch 900, Loss: 0.0977\n",
            "  Batch 1000, Loss: 0.1132\n",
            "  Batch 1100, Loss: 0.1690\n",
            "  Batch 1200, Loss: 0.1339\n",
            "  Batch 1300, Loss: 0.1785\n",
            "  Batch 1400, Loss: 0.0673\n",
            "  Batch 1500, Loss: 0.1445\n",
            "  Batch 1600, Loss: 0.2086\n",
            "  Batch 1700, Loss: 0.1093\n",
            "  Batch 1800, Loss: 0.2045\n",
            "Epoch 98 completed with average loss: 0.1827\n",
            "\n",
            "epoch99/100\n",
            "  Batch 0, Loss: 0.1166\n",
            "  Batch 100, Loss: 0.2957\n",
            "  Batch 200, Loss: 0.2095\n",
            "  Batch 300, Loss: 0.1489\n",
            "  Batch 400, Loss: 0.2169\n",
            "  Batch 500, Loss: 0.1165\n",
            "  Batch 600, Loss: 0.0600\n",
            "  Batch 700, Loss: 0.3002\n",
            "  Batch 800, Loss: 0.1192\n",
            "  Batch 900, Loss: 0.0528\n",
            "  Batch 1000, Loss: 0.0857\n",
            "  Batch 1100, Loss: 0.0993\n",
            "  Batch 1200, Loss: 0.1423\n",
            "  Batch 1300, Loss: 0.1644\n",
            "  Batch 1400, Loss: 0.1093\n",
            "  Batch 1500, Loss: 0.1680\n",
            "  Batch 1600, Loss: 0.2771\n",
            "  Batch 1700, Loss: 0.1350\n",
            "  Batch 1800, Loss: 0.1902\n",
            "Epoch 99 completed with average loss: 0.1815\n",
            "\n",
            "epoch100/100\n",
            "  Batch 0, Loss: 0.1802\n",
            "  Batch 100, Loss: 0.1474\n",
            "  Batch 200, Loss: 0.1365\n",
            "  Batch 300, Loss: 0.1107\n",
            "  Batch 400, Loss: 0.1368\n",
            "  Batch 500, Loss: 0.1761\n",
            "  Batch 600, Loss: 0.1703\n",
            "  Batch 700, Loss: 0.2586\n",
            "  Batch 800, Loss: 0.1203\n",
            "  Batch 900, Loss: 0.1460\n",
            "  Batch 1000, Loss: 0.2231\n",
            "  Batch 1100, Loss: 0.1014\n",
            "  Batch 1200, Loss: 0.1641\n",
            "  Batch 1300, Loss: 0.1714\n",
            "  Batch 1400, Loss: 0.2518\n",
            "  Batch 1500, Loss: 0.3689\n",
            "  Batch 1600, Loss: 0.1941\n",
            "  Batch 1700, Loss: 0.1850\n",
            "  Batch 1800, Loss: 0.0530\n",
            "Epoch 100 completed with average loss: 0.1798\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables to track test loss and correct predictions\n",
        "test_loss = 0\n",
        "correct_predictions = 0\n",
        "\n",
        "# No need for gradients during evaluation\n",
        "with torch.no_grad():\n",
        "    for X_test, y_test in test_loader:\n",
        "        # Forward pass: Get predictions\n",
        "        y_pred = model(X_test)\n",
        "\n",
        "        # Compute loss\n",
        "        test_loss += loss_fn(y_pred, y_test).item()\n",
        "\n",
        "        # Get the predicted class\n",
        "        predicted_classes = y_pred.argmax(dim=1)\n",
        "\n",
        "        # Count correct predictions\n",
        "        correct_predictions += (predicted_classes == y_test).sum().item()\n",
        "\n",
        "# Calculate average loss and accuracy\n",
        "average_test_loss = test_loss / len(test_loader)\n",
        "accuracy = correct_predictions / len(test_loader.dataset)\n",
        "\n",
        "print(f\"Test Loss: {average_test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixvkGbNIpCE3",
        "outputId": "7ea7104b-9ead-429a-8453-ae63a091e01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.3225\n",
            "Test Accuracy: 89.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model.state_dict(), \"binary_classification_model.pth\")\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "Wx5Owjs_Iq0F",
        "outputId": "e2df5e86-9ae5-4017-b116-2bb3c465c8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c59f8dde5729>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"binary_classification_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZtsPf9HPI-L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
